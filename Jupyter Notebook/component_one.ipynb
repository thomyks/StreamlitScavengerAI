{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d8efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing function definition added for compatibility\n",
    "def prepare_claude_prompt(column_samples: Dict[str, List[Any]], table_name: str) -> str:\n",
    "    \"\"\"Prepare prompt for Claude API with column samples\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are a database schema expert. Analyze the following dataset columns to generate optimized MySQL schema and business descriptions.\n",
    "\n",
    "Dataset: {table_name}\n",
    "\n",
    "For each column, I provide sample values. Generate the most appropriate MySQL-compatible SQL data type and a professional business description (max 400 chars).\n",
    "\n",
    "Column Analysis:\n",
    "\"\"\"\n",
    "    \n",
    "    for col_name, samples in column_samples.items():\n",
    "        prompt += f\"\"\"\n",
    "--- {col_name} ---\n",
    "Sample Values: {samples}\n",
    "\"\"\"\n",
    "    \n",
    "    prompt += \"\"\"\n",
    "\n",
    "Please provide your analysis in JSON format:\n",
    "{\n",
    "  \"columns\": {\n",
    "    \"column_name\": {\n",
    "      \"sql_type\": \"MySQL data type with size/precision\",\n",
    "      \"nullable\": true/false,\n",
    "      \"description\": \"Business description (max 400 chars)\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "Guidelines:\n",
    "- Use appropriate MySQL types: TINYINT, INT, BIGINT, DECIMAL(p,s), VARCHAR(n), DATE, BOOLEAN\n",
    "- Consider semantic meaning of column names\n",
    "- Write clear business descriptions\n",
    "- Use NOT NULL for columns that shouldn't be empty\"\"\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc4823",
   "metadata": {},
   "source": [
    "# Component One\n",
    "## Goal\n",
    "Automatically detect and parse CSV structures with comprehensive error handling, supporting diverse formatting edge cases.\n",
    "\n",
    "\n",
    "### Possible Challenges\n",
    "\n",
    "- Missing or ambiguous headers  \n",
    "- Different delimiter types  \n",
    "- Malformed rows  \n",
    "- Issues with heuristics for determining data types  \n",
    "\n",
    "\n",
    "### Success Criteria\n",
    "\n",
    "- **No data loss:** All rows and columns must be preserved, even if malformed (e.g., problematic rows are quarantined or logged).  \n",
    "- **Performance tracking:** Monitor and record processing time.  \n",
    "- **Traceability:** Maintain logs of errors and parsing issues.  \n",
    "\n",
    "\n",
    "### Technology\n",
    "\n",
    "- **Polars:** Chosen for its superior speed and memory efficiency compared to Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86550637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import polars as pl\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b138891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_duration(seconds: float) -> str:\n",
    "    if seconds < 1:\n",
    "        return f\"{seconds*1000:.1f}ms\"\n",
    "    elif seconds < 60:\n",
    "        return f\"{seconds:.2f}s\"\n",
    "    else:\n",
    "        minutes = int(seconds // 60)\n",
    "        remaining_seconds = seconds % 60\n",
    "        return f\"{minutes}m {remaining_seconds:.1f}s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a5e7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_file(file_path: Path, sample_lines: int = 5, use_random=False, max_random_lines=5000) -> Dict:\n",
    "    print(f\"ğŸ” Analyzing: {file_path.name}\")\n",
    "    lines = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "        all_lines = [line.strip() for _, line in zip(range(max_random_lines), f)]\n",
    "\n",
    "    # Filter out empty or comment lines\n",
    "    all_lines = [l for l in all_lines if l and not l.startswith(\"#\")]\n",
    "\n",
    "    if use_random:\n",
    "        lines = random.sample(all_lines, min(len(all_lines), sample_lines))\n",
    "    else:\n",
    "        lines = all_lines[:sample_lines]\n",
    "\n",
    "    if not lines:\n",
    "        return {'error': 'empty_file'}\n",
    "\n",
    "    first_line = lines[0]\n",
    "    delimiters = [',', ';', '\\t', '|']\n",
    "    counts = {d: first_line.count(d) for d in delimiters}\n",
    "    delimiter = max(counts, key=counts.get)\n",
    "    expected_cols = counts[delimiter] + 1\n",
    "\n",
    "    return {\n",
    "        'delimiter': delimiter,\n",
    "        'expected_columns': expected_cols,\n",
    "        'sample_lines': lines,\n",
    "        'likely_header': first_line.split(delimiter)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c0d7f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_fast(file_path: Path, delimiter: str) -> pl.DataFrame:\n",
    "    try:\n",
    "        df = pl.read_csv(\n",
    "            file_path, \n",
    "            separator=delimiter,\n",
    "            ignore_errors=True,\n",
    "            encoding=\"utf8-lossy\",\n",
    "            infer_schema_length=100\n",
    "        )\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "517f10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_merged_columns(df: pl.DataFrame, file_analysis: Dict) -> pl.DataFrame:\n",
    "    if df.shape[1] != 1:\n",
    "        return df\n",
    "    \n",
    "    col = df.columns[0]\n",
    "    data = df[col].to_list()\n",
    "    rows = [row.split(file_analysis['delimiter']) for row in data if row]\n",
    "    \n",
    "    headers = file_analysis['likely_header']\n",
    "    max_cols = max(len(row) for row in rows)\n",
    "    while len(headers) < max_cols:\n",
    "        headers.append(f\"COL_{len(headers)+1}\")\n",
    "    \n",
    "    col_data = {h: [r[i] if i < len(r) else None for r in rows] for i, h in enumerate(headers)}\n",
    "    return pl.DataFrame(col_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12ececb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string_columns(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    string_cols = [c for c in df.columns if df[c].dtype == pl.Utf8]\n",
    "    if not string_cols:\n",
    "        return df\n",
    "    \n",
    "    expressions = [\n",
    "        pl.col(c).str.strip_chars().replace(\"\", None).alias(c) if c in string_cols else pl.col(c)\n",
    "        for c in df.columns\n",
    "    ]\n",
    "    return df.select(expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e3c9afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataframe(df: pl.DataFrame, name=\"Dataset\"):\n",
    "    print(f\"\\nğŸ“Š {name.upper()}\")\n",
    "    print(f\"  âœ” Rows Ã— Columns: {df.shape}\")\n",
    "    print(f\"  âœ” Memory Usage: {df.estimated_size('mb'):.1f} MB\")\n",
    "\n",
    "    if df.shape[1] == 1:\n",
    "        print(\"  âš ï¸ Warning: Only one column detected â€” possible delimiter issue.\")\n",
    "    else:\n",
    "        print(f\"  âœ” Column Count OK: {df.shape[1]} columns\")\n",
    "\n",
    "    print(f\"  âœ” Column Types & Null %:\")\n",
    "    for i, col in enumerate(df.columns):\n",
    "        null_pct = (df[col].null_count() / df.shape[0]) * 100\n",
    "        dtype = str(df[col].dtype)\n",
    "        print(f\"    {i+1:2d}. {col:<20} {dtype:<10} Nulls: {null_pct:5.1f}%\")\n",
    "        if null_pct > 50:\n",
    "            print(f\"       âš ï¸ Warning: Over 50% nulls in column '{col}'\")\n",
    "\n",
    "    print(\"\\nğŸ“‹ Sample (first 3 rows):\")\n",
    "    try:\n",
    "        display(df.head(3))  # For notebooks\n",
    "    except:\n",
    "        print(df.head(3))    # Fallback for script mode\n",
    "\n",
    "    print(f\"âœ… {name.upper()} loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae9b7048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_file(file_path: str):\n",
    "    file_path = Path(file_path)\n",
    "    analysis = analyze_file(file_path)\n",
    "    \n",
    "    if 'error' in analysis:\n",
    "        print(f\"âŒ Skipping {file_path.name}: {analysis['error']}\")\n",
    "        return None\n",
    "    \n",
    "    df = load_csv_fast(file_path, analysis['delimiter'])\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    # Fix merged column if needed\n",
    "    if df.shape[1] == 1 and analysis['delimiter'] in df.columns[0]:\n",
    "        df = fix_merged_columns(df, analysis)\n",
    "    \n",
    "    df = clean_string_columns(df)\n",
    "    analyze_dataframe(df, name=file_path.stem)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3411b4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‚ Processing: sap\n",
      "ğŸ” Analyzing: sap.csv\n",
      "\n",
      "ğŸ“Š SAP\n",
      "  âœ” Rows Ã— Columns: (100000, 64)\n",
      "  âœ” Memory Usage: 25.5 MB\n",
      "  âœ” Column Count OK: 64 columns\n",
      "  âœ” Column Types & Null %:\n",
      "     1. VBELN                Int64      Nulls:   0.0%\n",
      "     2. GPAG                 Int64      Nulls:   0.0%\n",
      "     3. JPARVWGPAG           Int64      Nulls:   0.0%\n",
      "     4. ANGDT                Int64      Nulls:   0.0%\n",
      "     5. BNDDT                Int64      Nulls:   0.0%\n",
      "     6. VBTYP                String     Nulls:   0.0%\n",
      "     7. TRVOG                Int64      Nulls:   0.0%\n",
      "     8. AUART                String     Nulls:   0.0%\n",
      "     9. AUARTGRP             Int64      Nulls:   0.0%\n",
      "    10. WAERK                String     Nulls:   0.0%\n",
      "    11. NETWR                Float64    Nulls:   0.0%\n",
      "    12. VKORG                Int64      Nulls:   0.0%\n",
      "    13. VTWEG                String     Nulls:   0.0%\n",
      "    14. SPART                String     Nulls:   0.0%\n",
      "    15. VKGRP                String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'VKGRP'\n",
      "    16. VKBUR                String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'VKBUR'\n",
      "    17. EXEMPLART            String     Nulls:   0.2%\n",
      "    18. GSBER                String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'GSBER'\n",
      "    19. GSKST                String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'GSKST'\n",
      "    20. KNUMV                Int64      Nulls:   0.0%\n",
      "    21. FAKSP                String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'FAKSP'\n",
      "    22. KALSM                String     Nulls:   0.0%\n",
      "    23. KURST                String     Nulls:   0.0%\n",
      "    24. BSTNK                String     Nulls:  99.9%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'BSTNK'\n",
      "    25. BSARK                String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'BSARK'\n",
      "    26. BSTDK                Int64      Nulls:   0.0%\n",
      "    27. BSTZD                String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'BSTZD'\n",
      "    28. IHREZ                String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'IHREZ'\n",
      "    29. STAFO                String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'STAFO'\n",
      "    30. STWAE                String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'STWAE'\n",
      "    31. XIMMATRIK            String     Nulls:  99.9%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'XIMMATRIK'\n",
      "    32. IMMATDAT             Int64      Nulls:   0.0%\n",
      "    33. XSEPFKKOPF           String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'XSEPFKKOPF'\n",
      "    34. XREMRECHT            String     Nulls:  99.6%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'XREMRECHT'\n",
      "    35. REMSP                String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'REMSP'\n",
      "    36. KORRGRD              String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'KORRGRD'\n",
      "    37. REMVON               Int64      Nulls:   0.0%\n",
      "    38. REMBIS               Int64      Nulls:   0.0%\n",
      "    39. REFBELEG             String     Nulls:   1.5%\n",
      "    40. REMDATUM             Int64      Nulls:   0.0%\n",
      "    41. REMSCHEIN            Int64      Nulls:   0.0%\n",
      "    42. IVWDATUM             Int64      Nulls:   0.0%\n",
      "    43. POSNR_LAST           Int64      Nulls:   0.0%\n",
      "    44. POSEX_LAST           Int64      Nulls:   0.0%\n",
      "    45. KPOSN_LAST           Int64      Nulls:   0.0%\n",
      "    46. XFKBASAUFT           String     Nulls:   0.4%\n",
      "    47. XFKBASLIEF           String     Nulls:  99.6%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'XFKBASLIEF'\n",
      "    48. XJKSOFAKT            String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'XJKSOFAKT'\n",
      "    49. ERFUSER              String     Nulls:   0.0%\n",
      "    50. ERFDATE              Int64      Nulls:   0.0%\n",
      "    51. ERFTIME              Int64      Nulls:   0.0%\n",
      "    52. AENUSER              String     Nulls:   3.4%\n",
      "    53. AENDATE              Int64      Nulls:   0.0%\n",
      "    54. AENTIME              Int64      Nulls:   0.0%\n",
      "    55. XWBZABO              String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'XWBZABO'\n",
      "    56. XFKVDICHT            String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'XFKVDICHT'\n",
      "    57. XSTATARC             String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'XSTATARC'\n",
      "    58. XNOMESS              String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'XNOMESS'\n",
      "    59. KALSM_AMO            String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'KALSM_AMO'\n",
      "    60. XRENEWAL             String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'XRENEWAL'\n",
      "    61. AMORTN               String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'AMORTN'\n",
      "    62. REKLERGB             String     Nulls: 100.0%\n",
      "       âš ï¸ Warning: Over 50% nulls in column 'REKLERGB'\n",
      "    63. REKLTYP              Int64      Nulls:   0.0%\n",
      "    64. REKLDATUM            Int64      Nulls:   0.0%\n",
      "\n",
      "ğŸ“‹ Sample (first 3 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 64)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>VBELN</th><th>GPAG</th><th>JPARVWGPAG</th><th>ANGDT</th><th>BNDDT</th><th>VBTYP</th><th>TRVOG</th><th>AUART</th><th>AUARTGRP</th><th>WAERK</th><th>NETWR</th><th>VKORG</th><th>VTWEG</th><th>SPART</th><th>VKGRP</th><th>VKBUR</th><th>EXEMPLART</th><th>GSBER</th><th>GSKST</th><th>KNUMV</th><th>FAKSP</th><th>KALSM</th><th>KURST</th><th>BSTNK</th><th>BSARK</th><th>BSTDK</th><th>BSTZD</th><th>IHREZ</th><th>STAFO</th><th>STWAE</th><th>XIMMATRIK</th><th>IMMATDAT</th><th>XSEPFKKOPF</th><th>XREMRECHT</th><th>REMSP</th><th>KORRGRD</th><th>REMVON</th><th>REMBIS</th><th>REFBELEG</th><th>REMDATUM</th><th>REMSCHEIN</th><th>IVWDATUM</th><th>POSNR_LAST</th><th>POSEX_LAST</th><th>KPOSN_LAST</th><th>XFKBASAUFT</th><th>XFKBASLIEF</th><th>XJKSOFAKT</th><th>ERFUSER</th><th>ERFDATE</th><th>ERFTIME</th><th>AENUSER</th><th>AENDATE</th><th>AENTIME</th><th>XWBZABO</th><th>XFKVDICHT</th><th>XSTATARC</th><th>XNOMESS</th><th>KALSM_AMO</th><th>XRENEWAL</th><th>AMORTN</th><th>REKLERGB</th><th>REKLTYP</th><th>REKLDATUM</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>str</td><td>i64</td><td>str</td><td>f64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>31644090</td><td>93152002</td><td>1</td><td>0</td><td>0</td><td>&quot;K&quot;</td><td>7</td><td>&quot;GABO&quot;</td><td>6</td><td>&quot;EUR&quot;</td><td>0.0</td><td>210</td><td>&quot;AB&quot;</td><td>&quot;ZT&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>22448148</td><td>null</td><td>&quot;ZSTDA1&quot;</td><td>&quot;EURO&quot;</td><td>null</td><td>null</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>0</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>&quot;X&quot;</td><td>null</td><td>null</td><td>&quot;BURGERTB&quot;</td><td>20190715</td><td>74745</td><td>null</td><td>0</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1</td><td>20190715</td></tr><tr><td>31644091</td><td>96357515</td><td>1</td><td>0</td><td>0</td><td>&quot;K&quot;</td><td>7</td><td>&quot;GABO&quot;</td><td>6</td><td>&quot;EUR&quot;</td><td>-1.96</td><td>210</td><td>&quot;AB&quot;</td><td>&quot;ZT&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>22448149</td><td>null</td><td>&quot;ZSTDA1&quot;</td><td>&quot;EURO&quot;</td><td>null</td><td>null</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>0</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>&quot;X&quot;</td><td>null</td><td>null</td><td>&quot;SCHMIDSO&quot;</td><td>20190715</td><td>74745</td><td>&quot;SCHMIDSO&quot;</td><td>20190715</td><td>74914</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>4</td><td>20190715</td></tr><tr><td>31644092</td><td>93107637</td><td>1</td><td>0</td><td>0</td><td>&quot;K&quot;</td><td>7</td><td>&quot;GABO&quot;</td><td>6</td><td>&quot;EUR&quot;</td><td>0.0</td><td>210</td><td>&quot;AB&quot;</td><td>&quot;ZT&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>22448150</td><td>null</td><td>&quot;ZSTDA1&quot;</td><td>&quot;EURO&quot;</td><td>null</td><td>null</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0</td><td>0</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>&quot;X&quot;</td><td>null</td><td>null</td><td>&quot;KORNHAUSLER&quot;</td><td>20190715</td><td>74755</td><td>null</td><td>0</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1</td><td>20190715</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 64)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ VBELN    â”† GPAG     â”† JPARVWGPAG â”† ANGDT â”† â€¦ â”† AMORTN â”† REKLERGB â”† REKLTYP â”† REKLDATUM â”‚\n",
       "â”‚ ---      â”† ---      â”† ---        â”† ---   â”†   â”† ---    â”† ---      â”† ---     â”† ---       â”‚\n",
       "â”‚ i64      â”† i64      â”† i64        â”† i64   â”†   â”† str    â”† str      â”† i64     â”† i64       â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 31644090 â”† 93152002 â”† 1          â”† 0     â”† â€¦ â”† null   â”† null     â”† 1       â”† 20190715  â”‚\n",
       "â”‚ 31644091 â”† 96357515 â”† 1          â”† 0     â”† â€¦ â”† null   â”† null     â”† 4       â”† 20190715  â”‚\n",
       "â”‚ 31644092 â”† 93107637 â”† 1          â”† 0     â”† â€¦ â”† null   â”† null     â”† 1       â”† 20190715  â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SAP loaded successfully.\n",
      "\n",
      "ğŸ“‚ Processing: insurance\n",
      "ğŸ” Analyzing: insurance.csv\n",
      "\n",
      "ğŸ“Š INSURANCE\n",
      "  âœ” Rows Ã— Columns: (976, 6)\n",
      "  âœ” Memory Usage: 0.0 MB\n",
      "  âœ” Column Count OK: 6 columns\n",
      "  âœ” Column Types & Null %:\n",
      "     1. age                  Int64      Nulls:   0.0%\n",
      "     2. sex                  Int64      Nulls:   0.0%\n",
      "     3. bmi                  Float64    Nulls:   0.0%\n",
      "     4. children             Int64      Nulls:   0.0%\n",
      "     5. smoker               Int64      Nulls:   0.0%\n",
      "     6. charges              Float64    Nulls:   0.0%\n",
      "\n",
      "ğŸ“‹ Sample (first 3 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>age</th><th>sex</th><th>bmi</th><th>children</th><th>smoker</th><th>charges</th></tr><tr><td>i64</td><td>i64</td><td>f64</td><td>i64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>19</td><td>1</td><td>27.9</td><td>0</td><td>1</td><td>16884.924</td></tr><tr><td>18</td><td>0</td><td>33.77</td><td>1</td><td>0</td><td>1725.5523</td></tr><tr><td>28</td><td>0</td><td>33.0</td><td>3</td><td>0</td><td>4449.462</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 6)\n",
       "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ age â”† sex â”† bmi   â”† children â”† smoker â”† charges   â”‚\n",
       "â”‚ --- â”† --- â”† ---   â”† ---      â”† ---    â”† ---       â”‚\n",
       "â”‚ i64 â”† i64 â”† f64   â”† i64      â”† i64    â”† f64       â”‚\n",
       "â•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 19  â”† 1   â”† 27.9  â”† 0        â”† 1      â”† 16884.924 â”‚\n",
       "â”‚ 18  â”† 0   â”† 33.77 â”† 1        â”† 0      â”† 1725.5523 â”‚\n",
       "â”‚ 28  â”† 0   â”† 33.0  â”† 3        â”† 0      â”† 4449.462  â”‚\n",
       "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… INSURANCE loaded successfully.\n",
      "\n",
      "âœ… Completed in 136.5ms\n"
     ]
    }
   ],
   "source": [
    "file_paths = [\n",
    "    \"/Users/tomasnagy/Scavenger AI/sap.csv\",\n",
    "    \"/Users/tomasnagy/Scavenger AI/insurance.csv\"\n",
    "]\n",
    "\n",
    "datasets = {}\n",
    "start = time.time()\n",
    "\n",
    "for path in file_paths:\n",
    "    name = Path(path).stem\n",
    "    print(f\"\\nğŸ“‚ Processing: {name}\")\n",
    "    df = process_csv_file(path)\n",
    "    if df is not None:\n",
    "        datasets[name] = df\n",
    "\n",
    "print(f\"\\nâœ… Completed in {format_duration(time.time() - start)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114882d",
   "metadata": {},
   "source": [
    "# Component Two\n",
    "\n",
    "## Goal\n",
    "Analyze the data,  compute basic descriptive statistics with a focus on missing value treatment.\n",
    "\n",
    "### Possible Challenges\n",
    "\n",
    "    - Missing Values: analyse Develop a strategy for handling missing data.\n",
    "    - Decide how to select an imputation method (statistical vs. ML-based).\n",
    "    - Identify which columns require correction.\n",
    "    - Establish criteria to evaluate the effectiveness of the chosen strategy.\n",
    "\n",
    "- **Datatypes:**  \n",
    "    - Build heuristics for accurate data type identification.\n",
    "\n",
    "### Success Criteria\n",
    "\n",
    "- **Minimize Missing Data:**  \n",
    "    The proportion of missing data should be reduced as much as possible through appropriate handling and imputation.\n",
    "\n",
    "### Technology\n",
    "\n",
    "- **Polars:**  \n",
    "    Utilized for efficient data analysis and manipulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26ad6284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_structure_and_nulls(df: pl.DataFrame, name: str):\n",
    "    print(f\"\\nğŸ“Š {name.upper()}\")\n",
    "    print(\"â”€\" * 60)\n",
    "    \n",
    "    # Shape and Memory\n",
    "    print(f\"âœ”ï¸  Rows Ã— Columns:       {df.shape[0]} Ã— {df.shape[1]}\")\n",
    "    print(f\"âœ”ï¸  Memory Usage:         {df.estimated_size('mb'):.1f} MB\")\n",
    "\n",
    "    # Check for delimiter issue\n",
    "    if df.shape[1] == 1:\n",
    "        print(\"âš ï¸  Warning: Only one column detected â€” possible delimiter issue (e.g., expected ';' but read as ',').\")\n",
    "    else:\n",
    "        print(f\"âœ”ï¸  Column Count OK:      {df.shape[1]} columns\")\n",
    "\n",
    "    # Column overview with enhanced statistics\n",
    "    print(\"\\nğŸ“‘ Column Analysis:\")\n",
    "    numeric_cols = []\n",
    "    string_cols = []\n",
    "    \n",
    "    for i, col in enumerate(df.columns):\n",
    "        try:\n",
    "            null_count = df[col].null_count()\n",
    "            null_pct = (null_count / df.shape[0]) * 100\n",
    "            dtype = str(df[col].dtype)\n",
    "            \n",
    "            print(f\"  {i+1:>2}. {col:<25} {dtype:<12}  Nulls: {null_pct:5.1f}% ({null_count:,})\")\n",
    "            \n",
    "            # Collect column types for detailed stats\n",
    "            if df[col].dtype in [pl.Int64, pl.Int32, pl.Float64, pl.Float32]:\n",
    "                numeric_cols.append(col)\n",
    "            elif df[col].dtype == pl.Utf8:\n",
    "                string_cols.append(col)\n",
    "                \n",
    "            if null_pct > 50:\n",
    "                print(f\"      âš ï¸ Over 50% missing values in '{col}'\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ Error analyzing column '{col}': {e}\")\n",
    "\n",
    "    # Descriptive Statistics for Numeric Columns\n",
    "    if numeric_cols:\n",
    "        print(f\"\\nğŸ“ˆ NUMERIC COLUMNS STATISTICS ({len(numeric_cols)} columns)\")\n",
    "        print(\"â”€\" * 80)\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            try:\n",
    "                stats = df[col].describe()\n",
    "                non_null_count = df.shape[0] - df[col].null_count()\n",
    "                \n",
    "                print(f\"\\nğŸ”¢ {col.upper()}\")\n",
    "                print(f\"   Count (non-null): {non_null_count:,}\")\n",
    "                print(f\"   Mean:            {df[col].mean():.4f}\")\n",
    "                print(f\"   Std Dev:         {df[col].std():.4f}\")\n",
    "                print(f\"   Min:             {df[col].min()}\")\n",
    "                print(f\"   25th Percentile: {df[col].quantile(0.25)}\")\n",
    "                print(f\"   Median (50th):   {df[col].median()}\")\n",
    "                print(f\"   75th Percentile: {df[col].quantile(0.75)}\")\n",
    "                print(f\"   Max:             {df[col].max()}\")\n",
    "                \n",
    "                # Check for outliers using IQR method\n",
    "                q1 = df[col].quantile(0.25)\n",
    "                q3 = df[col].quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                lower_bound = q1 - 1.5 * iqr\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "                outliers = df.filter((pl.col(col) < lower_bound) | (pl.col(col) > upper_bound)).shape[0]\n",
    "                \n",
    "                if outliers > 0:\n",
    "                    print(f\"   âš ï¸ Potential outliers: {outliers:,} values outside [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Error calculating stats for '{col}': {e}\")\n",
    "\n",
    "    # String Column Analysis\n",
    "    if string_cols:\n",
    "        print(f\"\\nğŸ“ STRING COLUMNS ANALYSIS ({len(string_cols)} columns)\")\n",
    "        print(\"â”€\" * 80)\n",
    "        \n",
    "        for col in string_cols:\n",
    "            try:\n",
    "                non_null_count = df.shape[0] - df[col].null_count()\n",
    "                unique_count = df[col].n_unique()\n",
    "                \n",
    "                print(f\"\\nğŸ“„ {col.upper()}\")\n",
    "                print(f\"   Count (non-null):    {non_null_count:,}\")\n",
    "                print(f\"   Unique values:       {unique_count:,}\")\n",
    "                print(f\"   Uniqueness ratio:    {(unique_count/non_null_count)*100:.1f}%\")\n",
    "                \n",
    "                # Show most frequent values\n",
    "                if unique_count > 0:\n",
    "                    top_values = (df[col]\n",
    "                                .value_counts()\n",
    "                                .head(5)\n",
    "                                .sort('count', descending=True))\n",
    "                    \n",
    "                    print(\"   Top 5 values:\")\n",
    "                    for row in top_values.iter_rows():\n",
    "                        value, count = row\n",
    "                        pct = (count / non_null_count) * 100\n",
    "                        print(f\"     '{value}': {count:,} ({pct:.1f}%)\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Error analyzing string column '{col}': {e}\")\n",
    "\n",
    "    # Overall Data Quality Summary\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    total_nulls = sum(df[col].null_count() for col in df.columns)\n",
    "    completeness = ((total_cells - total_nulls) / total_cells) * 100\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ DATA QUALITY SUMMARY\")\n",
    "    print(\"â”€\" * 40)\n",
    "    print(f\"âœ”ï¸  Total cells:          {total_cells:,}\")\n",
    "    print(f\"âœ”ï¸  Non-null cells:       {total_cells - total_nulls:,}\")\n",
    "    print(f\"âœ”ï¸  Data completeness:    {completeness:.1f}%\")\n",
    "    print(f\"âœ”ï¸  Numeric columns:      {len(numeric_cols)}\")\n",
    "    print(f\"âœ”ï¸  String columns:       {len(string_cols)}\")\n",
    "\n",
    "    # Sample rows\n",
    "    print(\"\\nğŸ“‹ Sample (first 3 rows):\")\n",
    "    print(\"â”€\" * 60)\n",
    "    try:\n",
    "        sample = df.head(3)\n",
    "        header = \" | \".join(sample.columns)\n",
    "        types = \" | \".join(str(df[col].dtype) for col in sample.columns)\n",
    "        print(f\"| {header} |\")\n",
    "        print(f\"| {types} |\")\n",
    "        for row in sample.iter_rows():\n",
    "            print(\"| \" + \" | \".join(str(x) if x is not None else \"null\" for x in row) + \" |\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not print sample: {e}\")\n",
    "\n",
    "    print(f\"\\nâœ… {name.upper()} analysis completed.\")\n",
    "    print(\"â”€\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61b82265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‚ Processing: INSURANCE\n",
      "ğŸ” Analyzing: insurance.csv\n",
      "\n",
      "ğŸ“Š INSURANCE\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ”ï¸  Rows Ã— Columns:       976 Ã— 6\n",
      "âœ”ï¸  Memory Usage:         0.0 MB\n",
      "âœ”ï¸  Column Count OK:      6 columns\n",
      "\n",
      "ğŸ“‘ Column Analysis:\n",
      "   1. age                       Int64         Nulls:   0.0% (0)\n",
      "   2. sex                       Int64         Nulls:   0.0% (0)\n",
      "   3. bmi                       Float64       Nulls:   0.0% (0)\n",
      "   4. children                  Int64         Nulls:   0.0% (0)\n",
      "   5. smoker                    Int64         Nulls:   0.0% (0)\n",
      "   6. charges                   Float64       Nulls:   0.0% (0)\n",
      "\n",
      "ğŸ“ˆ NUMERIC COLUMNS STATISTICS (6 columns)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ”¢ AGE\n",
      "   Count (non-null): 976\n",
      "   Mean:            39.6670\n",
      "   Std Dev:         14.2353\n",
      "   Min:             18\n",
      "   25th Percentile: 27.0\n",
      "   Median (50th):   40.0\n",
      "   75th Percentile: 52.0\n",
      "   Max:             64\n",
      "\n",
      "ğŸ”¢ SEX\n",
      "   Count (non-null): 976\n",
      "   Mean:            0.4918\n",
      "   Std Dev:         0.5002\n",
      "   Min:             0\n",
      "   25th Percentile: 0.0\n",
      "   Median (50th):   0.0\n",
      "   75th Percentile: 1.0\n",
      "   Max:             1\n",
      "\n",
      "ğŸ”¢ BMI\n",
      "   Count (non-null): 976\n",
      "   Mean:            30.9088\n",
      "   Std Dev:         6.0470\n",
      "   Min:             15.96\n",
      "   25th Percentile: 26.6\n",
      "   Median (50th):   30.685\n",
      "   75th Percentile: 35.2\n",
      "   Max:             50.38\n",
      "   âš ï¸ Potential outliers: 2 values outside [13.70, 48.10]\n",
      "\n",
      "ğŸ”¢ CHILDREN\n",
      "   Count (non-null): 976\n",
      "   Mean:            1.0686\n",
      "   Std Dev:         1.1916\n",
      "   Min:             0\n",
      "   25th Percentile: 0.0\n",
      "   Median (50th):   1.0\n",
      "   75th Percentile: 2.0\n",
      "   Max:             5\n",
      "\n",
      "ğŸ”¢ SMOKER\n",
      "   Count (non-null): 976\n",
      "   Mean:            0.1977\n",
      "   Std Dev:         0.3985\n",
      "   Min:             0\n",
      "   25th Percentile: 0.0\n",
      "   Median (50th):   0.0\n",
      "   75th Percentile: 0.0\n",
      "   Max:             1\n",
      "   âš ï¸ Potential outliers: 193 values outside [0.00, 0.00]\n",
      "\n",
      "ğŸ”¢ CHARGES\n",
      "   Count (non-null): 976\n",
      "   Mean:            13147.9518\n",
      "   Std Dev:         12077.9053\n",
      "   Min:             1121.8739\n",
      "   25th Percentile: 4687.797\n",
      "   Median (50th):   9333.014350000001\n",
      "   75th Percentile: 16069.08475\n",
      "   Max:             63770.42801\n",
      "   âš ï¸ Potential outliers: 106 values outside [-12384.13, 33141.02]\n",
      "\n",
      "ğŸ“‹ DATA QUALITY SUMMARY\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ”ï¸  Total cells:          5,856\n",
      "âœ”ï¸  Non-null cells:       5,856\n",
      "âœ”ï¸  Data completeness:    100.0%\n",
      "âœ”ï¸  Numeric columns:      6\n",
      "âœ”ï¸  String columns:       0\n",
      "\n",
      "ğŸ“‹ Sample (first 3 rows):\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "| age | sex | bmi | children | smoker | charges |\n",
      "| Int64 | Int64 | Float64 | Int64 | Int64 | Float64 |\n",
      "| 19 | 1 | 27.9 | 0 | 1 | 16884.924 |\n",
      "| 18 | 0 | 33.77 | 1 | 0 | 1725.5523 |\n",
      "| 28 | 0 | 33.0 | 3 | 0 | 4449.462 |\n",
      "\n",
      "âœ… INSURANCE analysis completed.\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“‚ Processing: SAP\n",
      "ğŸ” Analyzing: sap.csv\n",
      "\n",
      "ğŸ“Š SAP\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ”ï¸  Rows Ã— Columns:       100000 Ã— 64\n",
      "âœ”ï¸  Memory Usage:         25.5 MB\n",
      "âœ”ï¸  Column Count OK:      64 columns\n",
      "\n",
      "ğŸ“‘ Column Analysis:\n",
      "   1. VBELN                     Int64         Nulls:   0.0% (0)\n",
      "   2. GPAG                      Int64         Nulls:   0.0% (0)\n",
      "   3. JPARVWGPAG                Int64         Nulls:   0.0% (3)\n",
      "   4. ANGDT                     Int64         Nulls:   0.0% (0)\n",
      "   5. BNDDT                     Int64         Nulls:   0.0% (0)\n",
      "   6. VBTYP                     String        Nulls:   0.0% (0)\n",
      "   7. TRVOG                     Int64         Nulls:   0.0% (13)\n",
      "   8. AUART                     String        Nulls:   0.0% (0)\n",
      "   9. AUARTGRP                  Int64         Nulls:   0.0% (0)\n",
      "  10. WAERK                     String        Nulls:   0.0% (0)\n",
      "  11. NETWR                     Float64       Nulls:   0.0% (0)\n",
      "  12. VKORG                     Int64         Nulls:   0.0% (0)\n",
      "  13. VTWEG                     String        Nulls:   0.0% (0)\n",
      "  14. SPART                     String        Nulls:   0.0% (0)\n",
      "  15. VKGRP                     String        Nulls: 100.0% (100,000)\n",
      "      âš ï¸ Over 50% missing values in 'VKGRP'\n",
      "  16. VKBUR                     String        Nulls: 100.0% (100,000)\n",
      "      âš ï¸ Over 50% missing values in 'VKBUR'\n",
      "  17. EXEMPLART                 String        Nulls:   0.2% (224)\n",
      "  18. GSBER                     String        Nulls: 100.0% (100,000)\n",
      "      âš ï¸ Over 50% missing values in 'GSBER'\n",
      "  19. GSKST                     String        Nulls: 100.0% (100,000)\n",
      "      âš ï¸ Over 50% missing values in 'GSKST'\n",
      "  20. KNUMV                     Int64         Nulls:   0.0% (16)\n",
      "  21. FAKSP                     String        Nulls: 100.0% (99,981)\n",
      "      âš ï¸ Over 50% missing values in 'FAKSP'\n",
      "  22. KALSM                     String        Nulls:   0.0% (0)\n",
      "  23. KURST                     String        Nulls:   0.0% (0)\n",
      "  24. BSTNK                     String        Nulls:  99.9% (99,930)\n",
      "      âš ï¸ Over 50% missing values in 'BSTNK'\n",
      "  25. BSARK                     String        Nulls: 100.0% (99,989)\n",
      "      âš ï¸ Over 50% missing values in 'BSARK'\n",
      "  26. BSTDK                     Int64         Nulls:   0.0% (0)\n",
      "  27. BSTZD                     String        Nulls: 100.0% (100,000)\n",
      "      âš ï¸ Over 50% missing values in 'BSTZD'\n",
      "  28. IHREZ                     String        Nulls: 100.0% (99,992)\n",
      "      âš ï¸ Over 50% missing values in 'IHREZ'\n",
      "  29. STAFO                     String        Nulls: 100.0% (100,000)\n",
      "      âš ï¸ Over 50% missing values in 'STAFO'\n",
      "  30. STWAE                     String        Nulls: 100.0% (100,000)\n",
      "      âš ï¸ Over 50% missing values in 'STWAE'\n",
      "  31. XIMMATRIK                 String        Nulls:  99.9% (99,852)\n",
      "      âš ï¸ Over 50% missing values in 'XIMMATRIK'\n",
      "  32. IMMATDAT                  Int64         Nulls:   0.0% (0)\n",
      "  33. XSEPFKKOPF                String        Nulls: 100.0% (99,997)\n",
      "      âš ï¸ Over 50% missing values in 'XSEPFKKOPF'\n",
      "  34. XREMRECHT                 String        Nulls:  99.6% (99,585)\n",
      "      âš ï¸ Over 50% missing values in 'XREMRECHT'\n",
      "  35. REMSP                     String        Nulls: 100.0% (100,000)\n",
      "      âš ï¸ Over 50% missing values in 'REMSP'\n",
      "  36. KORRGRD                   String        Nulls: 100.0% (100,000)\n",
      "      âš ï¸ Over 50% missing values in 'KORRGRD'\n",
      "  37. REMVON                    Int64         Nulls:   0.0% (0)\n",
      "  38. REMBIS                    Int64         Nulls:   0.0% (0)\n",
      "  39. REFBELEG                  String        Nulls:   1.5% (1,492)\n",
      "  40. REMDATUM                  Int64         Nulls:   0.0% (0)\n",
      "  41. REMSCHEIN                 Int64         Nulls:   0.0% (0)\n",
      "  42. IVWDATUM                  Int64         Nulls:   0.0% (0)\n",
      "  43. POSNR_LAST                Int64         Nulls:   0.0% (0)\n",
      "  44. POSEX_LAST                Int64         Nulls:   0.0% (0)\n",
      "  45. KPOSN_LAST                Int64         Nulls:   0.0% (0)\n",
      "  46. XFKBASAUFT                String        Nulls:   0.4% (418)\n",
      "  47. XFKBASLIEF                String        Nulls:  99.6% (99,582)\n",
      "      âš ï¸ Over 50% missing values in 'XFKBASLIEF'\n",
      "  48. XJKSOFAKT                 String        Nulls: 100.0% (99,999)\n",
      "      âš ï¸ Over 50% missing values in 'XJKSOFAKT'\n",
      "  49. ERFUSER                   String        Nulls:   0.0% (0)\n",
      "  50. ERFDATE                   Int64         Nulls:   0.0% (0)\n",
      "  51. ERFTIME                   Int64         Nulls:   0.0% (0)\n",
      "  52. AENUSER                   String        Nulls:   3.4% (3,364)\n",
      "  53. AENDATE                   Int64         Nulls:   0.0% (0)\n",
      "  54. AENTIME                   Int64         Nulls:   0.0% (0)\n",
      "  55. XWBZABO                   String        Nulls: 100.0% (100,000)\n",
      "      âš ï¸ Over 50% missing values in 'XWBZABO'\n",
      "  56. XFKVDICHT                 String        Nulls: 100.0% (100,000)\n",
      "      âš ï¸ Over 50% missing values in 'XFKVDICHT'\n",
      "  57. XSTATARC                  String        Nulls: 100.0% (100,000)\n",
      "      âš ï¸ Over 50% missing values in 'XSTATARC'\n",
      "  58. XNOMESS                   String        Nulls: 100.0% (100,000)\n",
      "      âš ï¸ Over 50% missing values in 'XNOMESS'\n",
      "  59. KALSM_AMO                 String        Nulls: 100.0% (100,000)\n",
      "      âš ï¸ Over 50% missing values in 'KALSM_AMO'\n",
      "  60. XRENEWAL                  String        Nulls: 100.0% (100,000)\n",
      "      âš ï¸ Over 50% missing values in 'XRENEWAL'\n",
      "  61. AMORTN                    String        Nulls: 100.0% (100,000)\n",
      "      âš ï¸ Over 50% missing values in 'AMORTN'\n",
      "  62. REKLERGB                  String        Nulls: 100.0% (99,988)\n",
      "      âš ï¸ Over 50% missing values in 'REKLERGB'\n",
      "  63. REKLTYP                   Int64         Nulls:   0.0% (0)\n",
      "  64. REKLDATUM                 Int64         Nulls:   0.0% (0)\n",
      "\n",
      "ğŸ“ˆ NUMERIC COLUMNS STATISTICS (26 columns)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ”¢ VBELN\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            481338.4215\n",
      "   Std Dev:         2002283.1110\n",
      "   Min:             28\n",
      "   25th Percentile: 67364.0\n",
      "   Median (50th):   134522.0\n",
      "   75th Percentile: 236733.0\n",
      "   Max:             31644113\n",
      "   âš ï¸ Potential outliers: 12,976 values outside [-186689.50, 490786.50]\n",
      "\n",
      "ğŸ”¢ GPAG\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            93206243.5648\n",
      "   Std Dev:         195978.7925\n",
      "   Min:             91000151\n",
      "   25th Percentile: 93143477.0\n",
      "   Median (50th):   93180409.5\n",
      "   75th Percentile: 93228256.0\n",
      "   Max:             96851553\n",
      "   âš ï¸ Potential outliers: 7,561 values outside [93016308.50, 93355424.50]\n",
      "\n",
      "ğŸ”¢ JPARVWGPAG\n",
      "   Count (non-null): 99,997\n",
      "   Mean:            1.0053\n",
      "   Std Dev:         0.0922\n",
      "   Min:             1\n",
      "   25th Percentile: 1.0\n",
      "   Median (50th):   1.0\n",
      "   75th Percentile: 1.0\n",
      "   Max:             5\n",
      "   âš ï¸ Potential outliers: 447 values outside [1.00, 1.00]\n",
      "\n",
      "ğŸ”¢ ANGDT\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            0.0000\n",
      "   Std Dev:         0.0000\n",
      "   Min:             0\n",
      "   25th Percentile: 0.0\n",
      "   Median (50th):   0.0\n",
      "   75th Percentile: 0.0\n",
      "   Max:             0\n",
      "\n",
      "ğŸ”¢ BNDDT\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            0.0000\n",
      "   Std Dev:         0.0000\n",
      "   Min:             0\n",
      "   25th Percentile: 0.0\n",
      "   Median (50th):   0.0\n",
      "   75th Percentile: 0.0\n",
      "   Max:             0\n",
      "\n",
      "ğŸ”¢ TRVOG\n",
      "   Count (non-null): 99,987\n",
      "   Mean:            0.0199\n",
      "   Std Dev:         0.3376\n",
      "   Min:             0\n",
      "   25th Percentile: 0.0\n",
      "   Median (50th):   0.0\n",
      "   75th Percentile: 0.0\n",
      "   Max:             8\n",
      "   âš ï¸ Potential outliers: 639 values outside [0.00, 0.00]\n",
      "\n",
      "ğŸ”¢ AUARTGRP\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            1.0153\n",
      "   Std Dev:         0.2442\n",
      "   Min:             1\n",
      "   25th Percentile: 1.0\n",
      "   Median (50th):   1.0\n",
      "   75th Percentile: 1.0\n",
      "   Max:             6\n",
      "   âš ï¸ Potential outliers: 639 values outside [1.00, 1.00]\n",
      "\n",
      "ğŸ”¢ NETWR\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            77.0913\n",
      "   Std Dev:         73.1088\n",
      "   Min:             -1123.82\n",
      "   25th Percentile: 52.9\n",
      "   Median (50th):   60.79\n",
      "   75th Percentile: 115.47\n",
      "   Max:             8022.7\n",
      "   âš ï¸ Potential outliers: 4,969 values outside [-40.96, 209.32]\n",
      "\n",
      "ğŸ”¢ VKORG\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            210.0000\n",
      "   Std Dev:         0.0000\n",
      "   Min:             210\n",
      "   25th Percentile: 210.0\n",
      "   Median (50th):   210.0\n",
      "   75th Percentile: 210.0\n",
      "   Max:             210\n",
      "\n",
      "ğŸ”¢ KNUMV\n",
      "   Count (non-null): 99,984\n",
      "   Mean:            217343.6879\n",
      "   Std Dev:         877972.4933\n",
      "   Min:             126916\n",
      "   25th Percentile: 152038.0\n",
      "   Median (50th):   177039.5\n",
      "   75th Percentile: 202760.0\n",
      "   Max:             22448176\n",
      "   âš ï¸ Potential outliers: 222 values outside [75955.00, 278843.00]\n",
      "\n",
      "ğŸ”¢ BSTDK\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            2612.7570\n",
      "   Std Dev:         229140.5672\n",
      "   Min:             0\n",
      "   25th Percentile: 0.0\n",
      "   Median (50th):   0.0\n",
      "   75th Percentile: 0.0\n",
      "   Max:             20220127\n",
      "   âš ï¸ Potential outliers: 13 values outside [0.00, 0.00]\n",
      "\n",
      "ğŸ”¢ IMMATDAT\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            29780.6370\n",
      "   Std Dev:         773545.1864\n",
      "   Min:             0\n",
      "   25th Percentile: 0.0\n",
      "   Median (50th):   0.0\n",
      "   75th Percentile: 0.0\n",
      "   Max:             20250331\n",
      "   âš ï¸ Potential outliers: 148 values outside [0.00, 0.00]\n",
      "\n",
      "ğŸ”¢ REMVON\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            603.3260\n",
      "   Std Dev:         110150.6572\n",
      "   Min:             0\n",
      "   25th Percentile: 0.0\n",
      "   Median (50th):   0.0\n",
      "   75th Percentile: 0.0\n",
      "   Max:             20110901\n",
      "   âš ï¸ Potential outliers: 3 values outside [0.00, 0.00]\n",
      "\n",
      "ğŸ”¢ REMBIS\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            603.3269\n",
      "   Std Dev:         110150.8179\n",
      "   Min:             0\n",
      "   25th Percentile: 0.0\n",
      "   Median (50th):   0.0\n",
      "   75th Percentile: 0.0\n",
      "   Max:             20110930\n",
      "   âš ï¸ Potential outliers: 3 values outside [0.00, 0.00]\n",
      "\n",
      "ğŸ”¢ REMDATUM\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            603.3279\n",
      "   Std Dev:         110150.9986\n",
      "   Min:             0\n",
      "   25th Percentile: 0.0\n",
      "   Median (50th):   0.0\n",
      "   75th Percentile: 0.0\n",
      "   Max:             20110930\n",
      "   âš ï¸ Potential outliers: 3 values outside [0.00, 0.00]\n",
      "\n",
      "ğŸ”¢ REMSCHEIN\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            0.0000\n",
      "   Std Dev:         0.0000\n",
      "   Min:             0\n",
      "   25th Percentile: 0.0\n",
      "   Median (50th):   0.0\n",
      "   75th Percentile: 0.0\n",
      "   Max:             0\n",
      "\n",
      "ğŸ”¢ IVWDATUM\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            603.3279\n",
      "   Std Dev:         110150.9986\n",
      "   Min:             0\n",
      "   25th Percentile: 0.0\n",
      "   Median (50th):   0.0\n",
      "   75th Percentile: 0.0\n",
      "   Max:             20110930\n",
      "   âš ï¸ Potential outliers: 3 values outside [0.00, 0.00]\n",
      "\n",
      "ğŸ”¢ POSNR_LAST\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            7.9924\n",
      "   Std Dev:         16.7073\n",
      "   Min:             0\n",
      "   25th Percentile: 1.0\n",
      "   Median (50th):   2.0\n",
      "   75th Percentile: 6.0\n",
      "   Max:             888\n",
      "   âš ï¸ Potential outliers: 14,928 values outside [-6.50, 13.50]\n",
      "\n",
      "ğŸ”¢ POSEX_LAST\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            10.4230\n",
      "   Std Dev:         3.1015\n",
      "   Min:             0\n",
      "   25th Percentile: 10.0\n",
      "   Median (50th):   10.0\n",
      "   75th Percentile: 10.0\n",
      "   Max:             120\n",
      "   âš ï¸ Potential outliers: 3,362 values outside [10.00, 10.00]\n",
      "\n",
      "ğŸ”¢ KPOSN_LAST\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            7.3681\n",
      "   Std Dev:         29.9208\n",
      "   Min:             0\n",
      "   25th Percentile: 1.0\n",
      "   Median (50th):   2.0\n",
      "   75th Percentile: 5.0\n",
      "   Max:             2862\n",
      "   âš ï¸ Potential outliers: 13,203 values outside [-5.00, 11.00]\n",
      "\n",
      "ğŸ”¢ ERFDATE\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            20010850.5839\n",
      "   Std Dev:         7088.3535\n",
      "   Min:             20010519\n",
      "   25th Percentile: 20010519.0\n",
      "   Median (50th):   20010520.0\n",
      "   75th Percentile: 20010520.0\n",
      "   Max:             20190715\n",
      "   âš ï¸ Potential outliers: 235 values outside [20010517.50, 20010521.50]\n",
      "\n",
      "ğŸ”¢ ERFTIME\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            136470.6527\n",
      "   Std Dev:         59015.5611\n",
      "   Min:             2\n",
      "   25th Percentile: 110718.0\n",
      "   Median (50th):   151024.0\n",
      "   75th Percentile: 173935.0\n",
      "   Max:             235951\n",
      "   âš ï¸ Potential outliers: 5,611 values outside [15892.50, 268760.50]\n",
      "\n",
      "ğŸ”¢ AENDATE\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            19421893.1450\n",
      "   Std Dev:         3624804.4421\n",
      "   Min:             0\n",
      "   25th Percentile: 20011231.0\n",
      "   Median (50th):   20050817.5\n",
      "   75th Percentile: 20200818.0\n",
      "   Max:             20241128\n",
      "   âš ï¸ Potential outliers: 3,364 values outside [19726850.50, 20485198.50]\n",
      "\n",
      "ğŸ”¢ AENTIME\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            132350.4117\n",
      "   Std Dev:         42855.7366\n",
      "   Min:             0\n",
      "   25th Percentile: 102727.0\n",
      "   Median (50th):   134848.0\n",
      "   75th Percentile: 163255.0\n",
      "   Max:             235947\n",
      "   âš ï¸ Potential outliers: 3,404 values outside [11935.00, 254047.00]\n",
      "\n",
      "ğŸ”¢ REKLTYP\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            0.0036\n",
      "   Std Dev:         0.0930\n",
      "   Min:             0\n",
      "   25th Percentile: 0.0\n",
      "   Median (50th):   0.0\n",
      "   75th Percentile: 0.0\n",
      "   Max:             4\n",
      "   âš ï¸ Potential outliers: 221 values outside [0.00, 0.00]\n",
      "\n",
      "ğŸ”¢ REKLDATUM\n",
      "   Count (non-null): 100,000\n",
      "   Mean:            44539.3840\n",
      "   Std Dev:         946391.1305\n",
      "   Min:             0\n",
      "   25th Percentile: 0.0\n",
      "   Median (50th):   0.0\n",
      "   75th Percentile: 0.0\n",
      "   Max:             20190715\n",
      "   âš ï¸ Potential outliers: 221 values outside [0.00, 0.00]\n",
      "\n",
      "ğŸ“ STRING COLUMNS ANALYSIS (38 columns)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“„ VBTYP\n",
      "   Count (non-null):    100,000\n",
      "   Unique values:       3\n",
      "   Uniqueness ratio:    0.0%\n",
      "   Top 5 values:\n",
      "     'C': 99,776 (99.8%)\n",
      "     'K': 221 (0.2%)\n",
      "     'H': 3 (0.0%)\n",
      "\n",
      "ğŸ“„ AUART\n",
      "   Count (non-null):    100,000\n",
      "   Unique values:       6\n",
      "   Uniqueness ratio:    0.0%\n",
      "   Top 5 values:\n",
      "     'ABO': 99,348 (99.3%)\n",
      "     'EV': 415 (0.4%)\n",
      "     'BV': 13 (0.0%)\n",
      "     'GEV': 5 (0.0%)\n",
      "     'REM': 3 (0.0%)\n",
      "\n",
      "ğŸ“„ WAERK\n",
      "   Count (non-null):    100,000\n",
      "   Unique values:       2\n",
      "   Uniqueness ratio:    0.0%\n",
      "   Top 5 values:\n",
      "     'EUR': 96,132 (96.1%)\n",
      "     'DEM': 3,868 (3.9%)\n",
      "\n",
      "ğŸ“„ VTWEG\n",
      "   Count (non-null):    100,000\n",
      "   Unique values:       2\n",
      "   Uniqueness ratio:    0.0%\n",
      "   Top 5 values:\n",
      "     'AB': 99,577 (99.6%)\n",
      "     'EV': 423 (0.4%)\n",
      "\n",
      "ğŸ“„ SPART\n",
      "   Count (non-null):    100,000\n",
      "   Unique values:       1\n",
      "   Uniqueness ratio:    0.0%\n",
      "   Top 5 values:\n",
      "     'ZT': 100,000 (100.0%)\n",
      "\n",
      "ğŸ“„ VKGRP\n",
      "   Count (non-null):    0\n",
      "   Unique values:       1\n",
      "   âŒ Error analyzing string column 'VKGRP': division by zero\n",
      "\n",
      "ğŸ“„ VKBUR\n",
      "   Count (non-null):    0\n",
      "   Unique values:       1\n",
      "   âŒ Error analyzing string column 'VKBUR': division by zero\n",
      "\n",
      "ğŸ“„ EXEMPLART\n",
      "   Count (non-null):    99,776\n",
      "   Unique values:       2\n",
      "   Uniqueness ratio:    0.0%\n",
      "   Top 5 values:\n",
      "     '0': 99,776 (100.0%)\n",
      "     'None': 224 (0.2%)\n",
      "\n",
      "ğŸ“„ GSBER\n",
      "   Count (non-null):    0\n",
      "   Unique values:       1\n",
      "   âŒ Error analyzing string column 'GSBER': division by zero\n",
      "\n",
      "ğŸ“„ GSKST\n",
      "   Count (non-null):    0\n",
      "   Unique values:       1\n",
      "   âŒ Error analyzing string column 'GSKST': division by zero\n",
      "\n",
      "ğŸ“„ FAKSP\n",
      "   Count (non-null):    19\n",
      "   Unique values:       2\n",
      "   Uniqueness ratio:    10.5%\n",
      "   Top 5 values:\n",
      "     'None': 99,981 (526215.8%)\n",
      "     'EV': 19 (100.0%)\n",
      "\n",
      "ğŸ“„ KALSM\n",
      "   Count (non-null):    100,000\n",
      "   Unique values:       3\n",
      "   Uniqueness ratio:    0.0%\n",
      "   Top 5 values:\n",
      "     'ZSTDA1': 99,564 (99.6%)\n",
      "     'ZSTDEV': 423 (0.4%)\n",
      "     'STDK00': 13 (0.0%)\n",
      "\n",
      "ğŸ“„ KURST\n",
      "   Count (non-null):    100,000\n",
      "   Unique values:       2\n",
      "   Uniqueness ratio:    0.0%\n",
      "   Top 5 values:\n",
      "     'EURO': 99,998 (100.0%)\n",
      "     'M': 2 (0.0%)\n",
      "\n",
      "ğŸ“„ BSTNK\n",
      "   Count (non-null):    70\n",
      "   Unique values:       65\n",
      "   Uniqueness ratio:    92.9%\n",
      "   Top 5 values:\n",
      "     'K-Nr.: 6002004': 2 (2.9%)\n",
      "     'BADENOVA-ID 169300': 1 (1.4%)\n",
      "     'BE17016220': 1 (1.4%)\n",
      "     '8282606.1': 1 (1.4%)\n",
      "     '8284578.1': 1 (1.4%)\n",
      "\n",
      "ğŸ“„ BSARK\n",
      "   Count (non-null):    11\n",
      "   Unique values:       3\n",
      "   Uniqueness ratio:    27.3%\n",
      "   Top 5 values:\n",
      "     'None': 99,989 (908990.9%)\n",
      "     'T': 6 (54.5%)\n",
      "     'P': 5 (45.5%)\n",
      "\n",
      "ğŸ“„ BSTZD\n",
      "   Count (non-null):    0\n",
      "   Unique values:       1\n",
      "   âŒ Error analyzing string column 'BSTZD': division by zero\n",
      "\n",
      "ğŸ“„ IHREZ\n",
      "   Count (non-null):    8\n",
      "   Unique values:       2\n",
      "   Uniqueness ratio:    25.0%\n",
      "   Top 5 values:\n",
      "     'None': 99,992 (1249900.0%)\n",
      "     'fau': 8 (100.0%)\n",
      "\n",
      "ğŸ“„ STAFO\n",
      "   Count (non-null):    0\n",
      "   Unique values:       1\n",
      "   âŒ Error analyzing string column 'STAFO': division by zero\n",
      "\n",
      "ğŸ“„ STWAE\n",
      "   Count (non-null):    0\n",
      "   Unique values:       1\n",
      "   âŒ Error analyzing string column 'STWAE': division by zero\n",
      "\n",
      "ğŸ“„ XIMMATRIK\n",
      "   Count (non-null):    148\n",
      "   Unique values:       2\n",
      "   Uniqueness ratio:    1.4%\n",
      "   Top 5 values:\n",
      "     'None': 99,852 (67467.6%)\n",
      "     'X': 148 (100.0%)\n",
      "\n",
      "ğŸ“„ XSEPFKKOPF\n",
      "   Count (non-null):    3\n",
      "   Unique values:       2\n",
      "   Uniqueness ratio:    66.7%\n",
      "   Top 5 values:\n",
      "     'None': 99,997 (3333233.3%)\n",
      "     'X': 3 (100.0%)\n",
      "\n",
      "ğŸ“„ XREMRECHT\n",
      "   Count (non-null):    415\n",
      "   Unique values:       2\n",
      "   Uniqueness ratio:    0.5%\n",
      "   Top 5 values:\n",
      "     'None': 99,585 (23996.4%)\n",
      "     'X': 415 (100.0%)\n",
      "\n",
      "ğŸ“„ REMSP\n",
      "   Count (non-null):    0\n",
      "   Unique values:       1\n",
      "   âŒ Error analyzing string column 'REMSP': division by zero\n",
      "\n",
      "ğŸ“„ KORRGRD\n",
      "   Count (non-null):    0\n",
      "   Unique values:       1\n",
      "   âŒ Error analyzing string column 'KORRGRD': division by zero\n",
      "\n",
      "ğŸ“„ REFBELEG\n",
      "   Count (non-null):    98,508\n",
      "   Unique values:       98,508\n",
      "   Uniqueness ratio:    100.0%\n",
      "   Top 5 values:\n",
      "     '0378170/0000/0161087': 1 (0.0%)\n",
      "     '0178379/0000/0064393': 1 (0.0%)\n",
      "     '0244996/0000/0115113': 1 (0.0%)\n",
      "     '0481599/0000/0236214': 1 (0.0%)\n",
      "     '0481720/0000/0236338': 1 (0.0%)\n",
      "\n",
      "ğŸ“„ XFKBASAUFT\n",
      "   Count (non-null):    99,582\n",
      "   Unique values:       2\n",
      "   Uniqueness ratio:    0.0%\n",
      "   Top 5 values:\n",
      "     'X': 99,582 (100.0%)\n",
      "     'None': 418 (0.4%)\n",
      "\n",
      "ğŸ“„ XFKBASLIEF\n",
      "   Count (non-null):    418\n",
      "   Unique values:       2\n",
      "   Uniqueness ratio:    0.5%\n",
      "   Top 5 values:\n",
      "     'None': 99,582 (23823.4%)\n",
      "     'X': 418 (100.0%)\n",
      "\n",
      "ğŸ“„ XJKSOFAKT\n",
      "   Count (non-null):    1\n",
      "   Unique values:       2\n",
      "   Uniqueness ratio:    200.0%\n",
      "   Top 5 values:\n",
      "     'None': 99,999 (9999900.0%)\n",
      "     'X': 1 (100.0%)\n",
      "\n",
      "ğŸ“„ ERFUSER\n",
      "   Count (non-null):    100,000\n",
      "   Unique values:       36\n",
      "   Uniqueness ratio:    0.0%\n",
      "   Top 5 values:\n",
      "     'HOFERERS': 92 (0.1%)\n",
      "     'BODEMERG': 91 (0.1%)\n",
      "     'PIAZZI': 12 (0.0%)\n",
      "     'JANNUSCHB': 12 (0.0%)\n",
      "     'BAURB': 2 (0.0%)\n",
      "\n",
      "ğŸ“„ AENUSER\n",
      "   Count (non-null):    96,636\n",
      "   Unique values:       445\n",
      "   Uniqueness ratio:    0.5%\n",
      "   Top 5 values:\n",
      "     'THOMAV': 133 (0.1%)\n",
      "     'SEKUL': 132 (0.1%)\n",
      "     'DÃ„SCHLEP': 126 (0.1%)\n",
      "     'MATAR': 21 (0.0%)\n",
      "     'HIRTM': 14 (0.0%)\n",
      "\n",
      "ğŸ“„ XWBZABO\n",
      "   Count (non-null):    0\n",
      "   Unique values:       1\n",
      "   âŒ Error analyzing string column 'XWBZABO': division by zero\n",
      "\n",
      "ğŸ“„ XFKVDICHT\n",
      "   Count (non-null):    0\n",
      "   Unique values:       1\n",
      "   âŒ Error analyzing string column 'XFKVDICHT': division by zero\n",
      "\n",
      "ğŸ“„ XSTATARC\n",
      "   Count (non-null):    0\n",
      "   Unique values:       1\n",
      "   âŒ Error analyzing string column 'XSTATARC': division by zero\n",
      "\n",
      "ğŸ“„ XNOMESS\n",
      "   Count (non-null):    0\n",
      "   Unique values:       1\n",
      "   âŒ Error analyzing string column 'XNOMESS': division by zero\n",
      "\n",
      "ğŸ“„ KALSM_AMO\n",
      "   Count (non-null):    0\n",
      "   Unique values:       1\n",
      "   âŒ Error analyzing string column 'KALSM_AMO': division by zero\n",
      "\n",
      "ğŸ“„ XRENEWAL\n",
      "   Count (non-null):    0\n",
      "   Unique values:       1\n",
      "   âŒ Error analyzing string column 'XRENEWAL': division by zero\n",
      "\n",
      "ğŸ“„ AMORTN\n",
      "   Count (non-null):    0\n",
      "   Unique values:       1\n",
      "   âŒ Error analyzing string column 'AMORTN': division by zero\n",
      "\n",
      "ğŸ“„ REKLERGB\n",
      "   Count (non-null):    12\n",
      "   Unique values:       2\n",
      "   Uniqueness ratio:    16.7%\n",
      "   Top 5 values:\n",
      "     'None': 99,988 (833233.3%)\n",
      "     '04': 12 (100.0%)\n",
      "\n",
      "ğŸ“‹ DATA QUALITY SUMMARY\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ”ï¸  Total cells:          6,400,000\n",
      "âœ”ï¸  Non-null cells:       3,795,575\n",
      "âœ”ï¸  Data completeness:    59.3%\n",
      "âœ”ï¸  Numeric columns:      26\n",
      "âœ”ï¸  String columns:       38\n",
      "\n",
      "ğŸ“‹ Sample (first 3 rows):\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "| VBELN | GPAG | JPARVWGPAG | ANGDT | BNDDT | VBTYP | TRVOG | AUART | AUARTGRP | WAERK | NETWR | VKORG | VTWEG | SPART | VKGRP | VKBUR | EXEMPLART | GSBER | GSKST | KNUMV | FAKSP | KALSM | KURST | BSTNK | BSARK | BSTDK | BSTZD | IHREZ | STAFO | STWAE | XIMMATRIK | IMMATDAT | XSEPFKKOPF | XREMRECHT | REMSP | KORRGRD | REMVON | REMBIS | REFBELEG | REMDATUM | REMSCHEIN | IVWDATUM | POSNR_LAST | POSEX_LAST | KPOSN_LAST | XFKBASAUFT | XFKBASLIEF | XJKSOFAKT | ERFUSER | ERFDATE | ERFTIME | AENUSER | AENDATE | AENTIME | XWBZABO | XFKVDICHT | XSTATARC | XNOMESS | KALSM_AMO | XRENEWAL | AMORTN | REKLERGB | REKLTYP | REKLDATUM |\n",
      "| Int64 | Int64 | Int64 | Int64 | Int64 | String | Int64 | String | Int64 | String | Float64 | Int64 | String | String | String | String | String | String | String | Int64 | String | String | String | String | String | Int64 | String | String | String | String | String | Int64 | String | String | String | String | Int64 | Int64 | String | Int64 | Int64 | Int64 | Int64 | Int64 | Int64 | String | String | String | String | Int64 | Int64 | String | Int64 | Int64 | String | String | String | String | String | String | String | String | Int64 | Int64 |\n",
      "| 31644090 | 93152002 | 1 | 0 | 0 | K | 7 | GABO | 6 | EUR | 0.0 | 210 | AB | ZT | null | null | null | null | null | 22448148 | null | ZSTDA1 | EURO | null | null | 0 | null | null | null | null | null | 0 | null | null | null | null | 0 | 0 | null | 0 | 0 | 0 | 0 | 0 | 1 | X | null | null | BURGERTB | 20190715 | 74745 | null | 0 | 0 | null | null | null | null | null | null | null | null | 1 | 20190715 |\n",
      "| 31644091 | 96357515 | 1 | 0 | 0 | K | 7 | GABO | 6 | EUR | -1.96 | 210 | AB | ZT | null | null | null | null | null | 22448149 | null | ZSTDA1 | EURO | null | null | 0 | null | null | null | null | null | 0 | null | null | null | null | 0 | 0 | null | 0 | 0 | 0 | 0 | 0 | 1 | X | null | null | SCHMIDSO | 20190715 | 74745 | SCHMIDSO | 20190715 | 74914 | null | null | null | null | null | null | null | null | 4 | 20190715 |\n",
      "| 31644092 | 93107637 | 1 | 0 | 0 | K | 7 | GABO | 6 | EUR | 0.0 | 210 | AB | ZT | null | null | null | null | null | 22448150 | null | ZSTDA1 | EURO | null | null | 0 | null | null | null | null | null | 0 | null | null | null | null | 0 | 0 | null | 0 | 0 | 0 | 0 | 0 | 1 | X | null | null | KORNHAUSLER | 20190715 | 74755 | null | 0 | 0 | null | null | null | null | null | null | null | null | 1 | 20190715 |\n",
      "\n",
      "âœ… SAP analysis completed.\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "âœ… All components completed in 1.06s\n"
     ]
    }
   ],
   "source": [
    "def process_csv_file_simple(file_path: str):\n",
    "    \"\"\"Simplified version without analysis output\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    analysis = analyze_file(file_path)\n",
    "    \n",
    "    if 'error' in analysis:\n",
    "        print(f\"âŒ Skipping {file_path.name}: {analysis['error']}\")\n",
    "        return None\n",
    "    \n",
    "    df = load_csv_fast(file_path, analysis['delimiter'])\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    # Fix merged column if needed\n",
    "    if df.shape[1] == 1 and analysis['delimiter'] in df.columns[0]:\n",
    "        df = fix_merged_columns(df, analysis)\n",
    "    \n",
    "    df = clean_string_columns(df)\n",
    "    return df\n",
    "\n",
    "# === Final Execution: Component One and Component Two ===\n",
    "file_paths = [\n",
    "    \"/Users/tomasnagy/Scavenger AI/insurance.csv\",\n",
    "    \"/Users/tomasnagy/Scavenger AI/sap.csv\"\n",
    "\n",
    "]\n",
    "\n",
    "datasets = {}\n",
    "start = time.time()\n",
    "\n",
    "for path in file_paths:\n",
    "    name = Path(path).stem\n",
    "    print(f\"\\nğŸ“‚ Processing: {name.upper()}\")\n",
    "    \n",
    "    # Component One: CSV Structure Detection (without analysis)\n",
    "    df = process_csv_file_simple(path)\n",
    "    \n",
    "    if df is not None:\n",
    "        datasets[name] = df\n",
    "        \n",
    "        # Component Two: Data Quality Analysis\n",
    "        analyze_dataset_structure_and_nulls(df, name)\n",
    "\n",
    "print(f\"\\nâœ… All components completed in {format_duration(time.time() - start)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4801a706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7270d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import time\n",
    "\n",
    "# Claude API configuration\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Claude API configuration\n",
    "CLAUDE_API_KEY = os.getenv('CLAUDE_API_KEY')\n",
    "if not CLAUDE_API_KEY:\n",
    "    raise ValueError(\n",
    "        \"ğŸ”‘ CLAUDE_API_KEY environment variable is not set!\\n\"\n",
    "        \"Please set it using: export CLAUDE_API_KEY='your-api-key-here'\\n\"\n",
    "        \"Or create a .env file with CLAUDE_API_KEY=your-api-key-here\"\n",
    "    )\n",
    "CLAUDE_API_URL = \"https://api.anthropic.com/v1/messages\"\n",
    "\n",
    "def analyze_column_metadata(df: pl.DataFrame) -> Dict[str, Dict]:\n",
    "    \"\"\"Analyze comprehensive metadata for each column using Polars\"\"\"\n",
    "    metadata = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_data = df[col]\n",
    "        non_null_data = col_data.drop_nulls()\n",
    "        \n",
    "        # Basic statistics\n",
    "        total_count = df.shape[0]\n",
    "        null_count = col_data.null_count()\n",
    "        non_null_count = total_count - null_count\n",
    "        null_percentage = (null_count / total_count) * 100 if total_count > 0 else 0\n",
    "        unique_count = col_data.n_unique()\n",
    "        \n",
    "        # Initialize column metadata\n",
    "        col_meta = {\n",
    "            'column_name': col,\n",
    "            'total_count': total_count,\n",
    "            'null_count': null_count,\n",
    "            'non_null_count': non_null_count,\n",
    "            'null_percentage': round(null_percentage, 2),\n",
    "            'unique_count': unique_count,\n",
    "            'uniqueness_ratio': round((unique_count / non_null_count) * 100, 2) if non_null_count > 0 else 0,\n",
    "            'polars_dtype': str(col_data.dtype),\n",
    "            'sample_values': [],\n",
    "            'most_common_value': None,\n",
    "            'most_common_count': 0,\n",
    "            'candidate_types': [],\n",
    "            'min_value': None,\n",
    "            'max_value': None,\n",
    "            'avg_length': None\n",
    "        }\n",
    "        \n",
    "        if non_null_count > 0:\n",
    "            # Sample values (up to 5)\n",
    "            sample_size = min(5, non_null_count)\n",
    "            if sample_size == non_null_count:\n",
    "                col_meta['sample_values'] = non_null_data.to_list()\n",
    "            else:\n",
    "                col_meta['sample_values'] = non_null_data.sample(sample_size).to_list()\n",
    "            \n",
    "            # Most common value\n",
    "            try:\n",
    "                value_counts = col_data.value_counts().head(1)\n",
    "                if value_counts.shape[0] > 0:\n",
    "                    row = value_counts.row(0)\n",
    "                    col_meta['most_common_value'] = row[0]\n",
    "                    col_meta['most_common_count'] = row[1]\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Type-specific analysis\n",
    "            if col_data.dtype in [pl.Int64, pl.Int32, pl.Int16, pl.Int8]:\n",
    "                col_meta['min_value'] = col_data.min()\n",
    "                col_meta['max_value'] = col_data.max()\n",
    "                col_meta['candidate_types'] = ['INT', 'BIGINT']\n",
    "                \n",
    "            elif col_data.dtype in [pl.Float64, pl.Float32]:\n",
    "                col_meta['min_value'] = col_data.min()\n",
    "                col_meta['max_value'] = col_data.max()\n",
    "                col_meta['candidate_types'] = ['DECIMAL', 'FLOAT', 'DOUBLE']\n",
    "                \n",
    "            elif col_data.dtype == pl.Utf8:\n",
    "                # String analysis\n",
    "                lengths = non_null_data.str.len_chars()\n",
    "                col_meta['min_value'] = lengths.min()\n",
    "                col_meta['max_value'] = lengths.max()\n",
    "                col_meta['avg_length'] = round(lengths.mean(), 1) if lengths.len() > 0 else 0\n",
    "                \n",
    "                # Detect candidate types for strings\n",
    "                candidates = ['VARCHAR']\n",
    "                \n",
    "                # Check if it could be a date (common patterns)\n",
    "                sample_str_values = [str(v) for v in col_meta['sample_values'][:3]]\n",
    "                if any(len(str(v)) == 8 and str(v).isdigit() for v in sample_str_values):\n",
    "                    candidates.append('DATE')  # YYYYMMDD format\n",
    "                elif any('/' in str(v) or '-' in str(v) for v in sample_str_values):\n",
    "                    candidates.append('DATE')  # Common date separators\n",
    "                \n",
    "                # Check if it could be boolean\n",
    "                unique_vals = set(str(v).upper() for v in col_meta['sample_values'])\n",
    "                if unique_vals.issubset({'0', '1', 'TRUE', 'FALSE', 'T', 'F', 'YES', 'NO', 'Y', 'N'}):\n",
    "                    candidates.append('BOOLEAN')\n",
    "                \n",
    "                # Check if it could be numeric (stored as string)\n",
    "                if all(str(v).replace('.', '').replace('-', '').isdigit() for v in sample_str_values if v):\n",
    "                    candidates.extend(['INT', 'DECIMAL'])\n",
    "                \n",
    "                col_meta['candidate_types'] = candidates\n",
    "                \n",
    "            elif col_data.dtype == pl.Boolean:\n",
    "                col_meta['candidate_types'] = ['BOOLEAN', 'TINYINT']\n",
    "                \n",
    "            else:\n",
    "                col_meta['candidate_types'] = ['VARCHAR']\n",
    "        \n",
    "        else:\n",
    "            # All null column\n",
    "            col_meta['sample_values'] = ['NULL']\n",
    "            col_meta['candidate_types'] = ['VARCHAR']\n",
    "        \n",
    "        metadata[col] = col_meta\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def extract_column_samples(df: pl.DataFrame, sample_size: int = 5) -> Dict[str, List[Any]]:\n",
    "    \"\"\"Extract sample values from each column for LLM analysis\"\"\"\n",
    "    samples = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Get non-null values first\n",
    "        non_null_values = df[col].drop_nulls()\n",
    "        \n",
    "        if non_null_values.len() > 0:  # Changed from .height to .len()\n",
    "            # Take sample_size random samples (or all if less than sample_size)\n",
    "            sample_count = min(sample_size, non_null_values.len())  # Changed from .height to .len()\n",
    "            if sample_count == non_null_values.len():  # Changed from .height to .len()\n",
    "                samples[col] = non_null_values.to_list()\n",
    "            else:\n",
    "                samples[col] = non_null_values.sample(sample_count).to_list()\n",
    "        else:\n",
    "            samples[col] = [\"NULL\"]  # Indicate all nulls\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def prepare_enhanced_claude_prompt(column_metadata: Dict[str, Dict], table_name: str) -> str:\n",
    "    \"\"\"Prepare an enhanced prompt with rich metadata for Claude to analyze column types and generate descriptions\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are a database schema expert. Analyze the following dataset columns with their comprehensive metadata to generate optimized SQL schema and business descriptions.\n",
    "\n",
    "Dataset: {table_name}\n",
    "\n",
    "For each column below, I provide:\n",
    "- Column name and semantic context\n",
    "- Sample values\n",
    "- Statistical metadata (nulls, uniqueness, min/max, most common value)\n",
    "- Candidate data types detected by analysis\n",
    "- Current Polars data type\n",
    "\n",
    "Your task: Generate the most appropriate MySQL-compatible SQL data type and a professional business description.\n",
    "\n",
    "Column Analysis:\n",
    "\"\"\"\n",
    "    \n",
    "    for col_name, meta in column_metadata.items():\n",
    "        prompt += f\"\"\"\n",
    "--- {col_name} ---\n",
    "- Sample Values: {meta['sample_values']}\n",
    "- Most Common: {meta['most_common_value']} (appears {meta['most_common_count']} times)\n",
    "- Nulls: {meta['null_percentage']}% ({meta['null_count']}/{meta['total_count']})\n",
    "- Unique Values: {meta['unique_count']} ({meta['uniqueness_ratio']}% unique)\n",
    "- Candidate Types: {meta['candidate_types']}\n",
    "- Current Type: {meta['polars_dtype']}\"\"\"\n",
    "        \n",
    "        if meta['min_value'] is not None and meta['max_value'] is not None:\n",
    "            if meta['polars_dtype'] == 'Utf8':\n",
    "                prompt += f\"\\n- String Length: min={meta['min_value']}, max={meta['max_value']}, avg={meta['avg_length']}\"\n",
    "            else:\n",
    "                prompt += f\"\\n- Value Range: {meta['min_value']} to {meta['max_value']}\"\n",
    "    \n",
    "    prompt += \"\"\"\n",
    "\n",
    "Please provide your analysis in the following JSON format:\n",
    "{\n",
    "  \"columns\": {\n",
    "    \"column_name\": {\n",
    "      \"sql_type\": \"optimized MySQL data type with proper size/precision\",\n",
    "      \"nullable\": true/false,\n",
    "      \"description\": \"Professional business description based on semantic analysis\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "Guidelines for optimal SQL types:\n",
    "- Use TINYINT for small integers (0-255)\n",
    "- Use INT for standard integers \n",
    "- Use BIGINT for large integers\n",
    "- Use DECIMAL(precision,scale) for exact decimals (e.g., DECIMAL(10,2) for currency)\n",
    "- Use FLOAT/DOUBLE only for scientific data\n",
    "- Use VARCHAR(optimal_length) based on max string length + buffer\n",
    "- Use DATE for date values (YYYYMMDD format should be DATE, not INT)\n",
    "- Use BOOLEAN/TINYINT(1) for binary flags\n",
    "- Consider NOT NULL for columns with 0% nulls and clear business requirements\n",
    "- Use semantic understanding: 'user_id' suggests primary key, 'email' suggests unique varchar, 'created_at' suggests timestamp\n",
    "\n",
    "For descriptions:\n",
    "- Analyze column name semantically (e.g., 'VBELN' might be 'Sales Document Number')\n",
    "- Use sample values to understand the business context\n",
    "- Write professional, clear descriptions explaining business purpose\n",
    "- Consider data patterns (e.g., all values start with same prefix = identifier/code)\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def call_claude_api_robust(prompt: str, max_retries: int = 3, model: str = \"claude-3-sonnet-20240229\") -> Dict:\n",
    "    \"\"\"Enhanced Claude API call with retries, timeouts, and exponential backoff\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"x-api-key\": CLAUDE_API_KEY,\n",
    "        \"anthropic-version\": \"2023-06-01\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 4000,  # Increased for larger schemas\n",
    "        \"temperature\": 0.1,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"ğŸ“¡ Attempt {attempt + 1}/{max_retries}: Calling Claude API...\")\n",
    "            \n",
    "            # Make request with timeout\n",
    "            response = requests.post(\n",
    "                CLAUDE_API_URL, \n",
    "                headers=headers, \n",
    "                json=data, \n",
    "                timeout=120  # 2 minute timeout\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            content = result[\"content\"][0][\"text\"]\n",
    "            \n",
    "            # Handle markdown code blocks - remove ```json and ``` if present\n",
    "            if \"```json\" in content:\n",
    "                content = content.split(\"```json\")[1]\n",
    "                if \"```\" in content:\n",
    "                    content = content.split(\"```\")[0]\n",
    "            elif \"```\" in content and content.count(\"```\") >= 2:\n",
    "                # Handle generic code blocks\n",
    "                parts = content.split(\"```\")\n",
    "                if len(parts) >= 3:\n",
    "                    content = parts[1]\n",
    "            \n",
    "            # Extract JSON from the response\n",
    "            start_idx = content.find('{')\n",
    "            end_idx = content.rfind('}') + 1\n",
    "            \n",
    "            if start_idx != -1 and end_idx > start_idx:\n",
    "                json_str = content[start_idx:end_idx].strip()\n",
    "                parsed_json = json.loads(json_str)\n",
    "                print(f\"âœ… Successfully parsed response on attempt {attempt + 1}\")\n",
    "                return parsed_json\n",
    "            else:\n",
    "                print(f\"âš ï¸ Attempt {attempt + 1}: Could not find JSON in Claude response\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(f\"Raw content preview: {content[:200]}...\")\n",
    "                    return None\n",
    "                    \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"â° Attempt {attempt + 1}: Request timeout (120s exceeded)\")\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s, 4s\n",
    "                print(f\"â³ Waiting {wait_time}s before retry...\")\n",
    "                time.sleep(wait_time)\n",
    "                \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"ğŸŒ Attempt {attempt + 1}: Connection error\")\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt\n",
    "                print(f\"â³ Waiting {wait_time}s before retry...\")\n",
    "                time.sleep(wait_time)\n",
    "                \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"ğŸš« Attempt {attempt + 1}: HTTP error {e.response.status_code}\")\n",
    "            if e.response.status_code == 429:  # Rate limit\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 10 + (2 ** attempt)  # Longer wait for rate limits\n",
    "                    print(f\"ğŸš¦ Rate limited. Waiting {wait_time}s before retry...\")\n",
    "                    time.sleep(wait_time)\n",
    "            elif e.response.status_code >= 500:  # Server error\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 5 + (2 ** attempt)\n",
    "                    print(f\"ğŸ”§ Server error. Waiting {wait_time}s before retry...\")\n",
    "                    time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"âŒ Non-retryable HTTP error: {e}\")\n",
    "                return None\n",
    "                \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"ğŸ“„ Attempt {attempt + 1}: JSON parse error - {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Final JSON content preview: {content[:300] if 'content' in locals() else 'No content'}...\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Attempt {attempt + 1}: Unexpected error - {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                import traceback\n",
    "                print(f\"Full traceback: {traceback.format_exc()}\")\n",
    "                return None\n",
    "    \n",
    "    print(f\"ğŸ’¥ All {max_retries} attempts failed\")\n",
    "    return None\n",
    "\n",
    "# Legacy function for backward compatibility\n",
    "def call_claude_api(prompt: str, model: str = \"claude-3-sonnet-20240229\") -> Dict:\n",
    "    \"\"\"Legacy function - now uses robust version\"\"\"\n",
    "    return call_claude_api_robust(prompt, max_retries=3, model=model)\n",
    "\n",
    "def generate_ddl_from_schema(schema_data: Dict, table_name: str) -> str:\n",
    "    \"\"\"Generate CREATE TABLE DDL from Claude schema analysis\"\"\"\n",
    "    \n",
    "    if not schema_data or 'columns' not in schema_data:\n",
    "        return \"-- Error: Invalid schema data\"\n",
    "    \n",
    "    ddl = f\"CREATE TABLE {table_name} (\\n\"\n",
    "    \n",
    "    column_definitions = []\n",
    "    for col_name, col_info in schema_data['columns'].items():\n",
    "        sql_type = col_info.get('sql_type', 'VARCHAR(255)')\n",
    "        nullable = col_info.get('nullable', True)\n",
    "        \n",
    "        null_constraint = \" NOT NULL\" if not nullable else \"\"\n",
    "        column_def = f\"  `{col_name}` {sql_type}{null_constraint}\"\n",
    "        column_definitions.append(column_def)\n",
    "    \n",
    "    ddl += \",\\n\".join(column_definitions)\n",
    "    ddl += \"\\n);\"\n",
    "    \n",
    "    return ddl\n",
    "\n",
    "def generate_column_descriptions(schema_data: Dict) -> str:\n",
    "    \"\"\"Generate JSON formatted column descriptions\"\"\"\n",
    "    \n",
    "    if not schema_data or 'columns' not in schema_data:\n",
    "        return \"{}\"\n",
    "    \n",
    "    descriptions = {}\n",
    "    for col_name, col_info in schema_data['columns'].items():\n",
    "        descriptions[col_name] = col_info.get('description', f\"Description for {col_name}\")\n",
    "    \n",
    "    return json.dumps(descriptions, indent=2)\n",
    "\n",
    "def analyze_schema_with_claude(df: pl.DataFrame, table_name: str, sample_size: int = 5) -> Tuple[str, str]:\n",
    "    \"\"\"Complete pipeline: extract samples, analyze with Claude, generate DDL and descriptions\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ” Analyzing schema for {table_name} using Claude...\")\n",
    "    \n",
    "    # Step 1: Extract column samples\n",
    "    column_samples = extract_column_samples(df, sample_size)\n",
    "    print(f\"âœ”ï¸ Extracted samples from {len(column_samples)} columns\")\n",
    "    \n",
    "    # Step 2: Prepare Claude prompt\n",
    "    prompt = prepare_claude_prompt(column_samples, table_name)\n",
    "    \n",
    "    # Step 3: Call Claude for analysis with robust error handling\n",
    "    print(\"ğŸ¤– Calling Claude API for schema analysis...\")\n",
    "    schema_data = call_claude_api_robust(prompt)\n",
    "    \n",
    "    if not schema_data:\n",
    "        return \"-- Error: Failed to generate schema after multiple attempts\", \"{}\"\n",
    "    \n",
    "    # Step 4: Generate DDL and descriptions\n",
    "    ddl = generate_ddl_from_schema(schema_data, table_name)\n",
    "    descriptions = generate_column_descriptions(schema_data)\n",
    "    \n",
    "    print(f\"âœ… Schema analysis completed for {table_name}\")\n",
    "    \n",
    "    return ddl, descriptions\n",
    "\n",
    "def display_schema_results(ddl: str, descriptions: str, table_name: str):\n",
    "    \"\"\"Display the generated schema and descriptions in a formatted way\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ CLAUDE-GENERATED SCHEMA FOR {table_name.upper()}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\nğŸ—‚ï¸ DDL (Data Definition Language):\")\n",
    "    print(\"â”€\" * 50)\n",
    "    print(ddl)\n",
    "    \n",
    "    print(\"\\nğŸ“– Column Descriptions:\")\n",
    "    print(\"â”€\" * 50)\n",
    "    print(descriptions)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "def enhanced_analysis_with_claude_schema(df: pl.DataFrame, name: str):\n",
    "    \"\"\"Enhanced analysis that includes Claude-based schema generation\"\"\"\n",
    "    \n",
    "    # First run your existing analysis\n",
    "    analyze_dataset_structure_and_nulls(df, name)\n",
    "    \n",
    "    # Then add Claude schema analysis\n",
    "    ddl, descriptions = analyze_schema_with_claude(df, name)\n",
    "    display_schema_results(ddl, descriptions, name)\n",
    "    \n",
    "    return ddl, descriptions\n",
    "\n",
    "def test_claude_connection() -> bool:\n",
    "    \"\"\"Test if Claude API is accessible with robust error handling\"\"\"\n",
    "    test_prompt = \"Please respond with just the word 'SUCCESS' if you can read this.\"\n",
    "    \n",
    "    print(\"ğŸ”— Testing Claude API connection...\")\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"x-api-key\": CLAUDE_API_KEY,\n",
    "        \"anthropic-version\": \"2023-06-01\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"claude-3-sonnet-20240229\",\n",
    "        \"max_tokens\": 10,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(CLAUDE_API_URL, headers=headers, json=data, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        content = result[\"content\"][0][\"text\"]\n",
    "        \n",
    "        if \"SUCCESS\" in content:\n",
    "            print(\"âœ… Claude API connection successful\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âš ï¸ Claude API responded but with unexpected content: {content}\")\n",
    "            return False\n",
    "            \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"âŒ Claude API connection failed: Timeout\")\n",
    "        return False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"âŒ Claude API connection failed: Connection error\")\n",
    "        return False\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"âŒ Claude API connection failed: HTTP {e.response.status_code}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Claude API connection failed: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "531893a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing Schema Generation...\n",
      "âœ… Claude API connection successful\n",
      "\n",
      "ğŸ” Testing schema generation with 'insurance' dataset...\n",
      "ğŸ” Analyzing schema for insurance using Claude...\n",
      "âœ”ï¸ Extracted samples from 6 columns\n",
      "ğŸ¤– Calling Claude API for schema analysis...\n",
      "âœ… Claude API connection successful\n",
      "\n",
      "ğŸ” Testing schema generation with 'insurance' dataset...\n",
      "ğŸ” Analyzing schema for insurance using Claude...\n",
      "âœ”ï¸ Extracted samples from 6 columns\n",
      "ğŸ¤– Calling Claude API for schema analysis...\n",
      "âœ… Schema analysis completed for insurance\n",
      "\n",
      "ğŸ“‹ CLAUDE-GENERATED SCHEMA FOR INSURANCE\n",
      "======================================================================\n",
      "\n",
      "ğŸ—‚ï¸ DDL (Data Definition Language):\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "CREATE TABLE insurance (\n",
      "  `age` INT NOT NULL,\n",
      "  `sex` BOOLEAN NOT NULL,\n",
      "  `bmi` DECIMAL(5,2) NOT NULL,\n",
      "  `children` INT NOT NULL,\n",
      "  `smoker` BOOLEAN NOT NULL,\n",
      "  `charges` DECIMAL(10,4) NOT NULL\n",
      ");\n",
      "\n",
      "ğŸ“– Column Descriptions:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "{\n",
      "  \"age\": \"The age of the insured person in years.\",\n",
      "  \"sex\": \"The biological sex of the insured person, where 0 represents female and 1 represents male.\",\n",
      "  \"bmi\": \"The body mass index (BMI) of the insured person, calculated as weight in kilograms divided by the square of height in meters.\",\n",
      "  \"children\": \"The number of children the insured person has.\",\n",
      "  \"smoker\": \"Indicates whether the insured person is a smoker or not, where 0 represents a non-smoker and 1 represents a smoker.\",\n",
      "  \"charges\": \"The total insurance charges or costs incurred by the insured person.\"\n",
      "}\n",
      "\n",
      "======================================================================\n",
      "âœ… Schema analysis completed for insurance\n",
      "\n",
      "ğŸ“‹ CLAUDE-GENERATED SCHEMA FOR INSURANCE\n",
      "======================================================================\n",
      "\n",
      "ğŸ—‚ï¸ DDL (Data Definition Language):\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "CREATE TABLE insurance (\n",
      "  `age` INT NOT NULL,\n",
      "  `sex` BOOLEAN NOT NULL,\n",
      "  `bmi` DECIMAL(5,2) NOT NULL,\n",
      "  `children` INT NOT NULL,\n",
      "  `smoker` BOOLEAN NOT NULL,\n",
      "  `charges` DECIMAL(10,4) NOT NULL\n",
      ");\n",
      "\n",
      "ğŸ“– Column Descriptions:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "{\n",
      "  \"age\": \"The age of the insured person in years.\",\n",
      "  \"sex\": \"The biological sex of the insured person, where 0 represents female and 1 represents male.\",\n",
      "  \"bmi\": \"The body mass index (BMI) of the insured person, calculated as weight in kilograms divided by the square of height in meters.\",\n",
      "  \"children\": \"The number of children the insured person has.\",\n",
      "  \"smoker\": \"Indicates whether the insured person is a smoker or not, where 0 represents a non-smoker and 1 represents a smoker.\",\n",
      "  \"charges\": \"The total insurance charges or costs incurred by the insured person.\"\n",
      "}\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def extract_column_samples(df: pl.DataFrame, sample_size: int = 5) -> Dict[str, List[Any]]:\n",
    "    \"\"\"Extract sample values from each column for LLM analysis\"\"\"\n",
    "    samples = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Get non-null values first\n",
    "        non_null_values = df[col].drop_nulls()\n",
    "        \n",
    "        if non_null_values.len() > 0:  # Changed from .height to .len()\n",
    "            # Take sample_size random samples (or all if less than sample_size)\n",
    "            sample_count = min(sample_size, non_null_values.len())  # Changed from .height to .len()\n",
    "            if sample_count == non_null_values.len():  # Changed from .height to .len()\n",
    "                samples[col] = non_null_values.to_list()\n",
    "            else:\n",
    "                samples[col] = non_null_values.sample(sample_count).to_list()\n",
    "        else:\n",
    "            samples[col] = [\"NULL\"]  # Indicate all nulls\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Test the schema generation functionality\n",
    "print(\"ğŸ§ª Testing Schema Generation...\")\n",
    "\n",
    "# Test Claude connection first\n",
    "test_claude_connection()\n",
    "\n",
    "# Test with one of the loaded datasets\n",
    "if 'datasets' in globals() and datasets:\n",
    "    dataset_name = list(datasets.keys())[0]\n",
    "    test_df = datasets[dataset_name]\n",
    "    \n",
    "    print(f\"\\nğŸ” Testing schema generation with '{dataset_name}' dataset...\")\n",
    "    ddl, descriptions = analyze_schema_with_claude(test_df, dataset_name, sample_size=3)\n",
    "    \n",
    "    display_schema_results(ddl, descriptions, dataset_name)\n",
    "else:\n",
    "    print(\"âš ï¸ No datasets loaded. Please run the CSV processing cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a3193e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing Schema Generation...\n",
      "âœ… Claude API connection successful\n",
      "\n",
      "ğŸ” Testing schema generation with 'insurance' dataset...\n",
      "ğŸ” Analyzing schema for insurance using Claude...\n",
      "âœ”ï¸ Extracted samples from 6 columns\n",
      "ğŸ¤– Calling Claude API for schema analysis...\n",
      "âœ… Claude API connection successful\n",
      "\n",
      "ğŸ” Testing schema generation with 'insurance' dataset...\n",
      "ğŸ” Analyzing schema for insurance using Claude...\n",
      "âœ”ï¸ Extracted samples from 6 columns\n",
      "ğŸ¤– Calling Claude API for schema analysis...\n",
      "âœ… Schema analysis completed for insurance\n",
      "\n",
      "ğŸ“‹ CLAUDE-GENERATED SCHEMA FOR INSURANCE\n",
      "======================================================================\n",
      "\n",
      "ğŸ—‚ï¸ DDL (Data Definition Language):\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "CREATE TABLE insurance (\n",
      "  `age` INT NOT NULL,\n",
      "  `sex` BOOLEAN NOT NULL,\n",
      "  `bmi` DECIMAL(5,2) NOT NULL,\n",
      "  `children` INT NOT NULL,\n",
      "  `smoker` BOOLEAN NOT NULL,\n",
      "  `charges` DECIMAL(10,5) NOT NULL\n",
      ");\n",
      "\n",
      "ğŸ“– Column Descriptions:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "{\n",
      "  \"age\": \"The age of the insured person in years.\",\n",
      "  \"sex\": \"The biological sex of the insured person, where 0 represents female and 1 represents male.\",\n",
      "  \"bmi\": \"The body mass index (BMI) of the insured person, calculated as weight in kilograms divided by the square of height in meters.\",\n",
      "  \"children\": \"The number of children the insured person has.\",\n",
      "  \"smoker\": \"Indicates whether the insured person is a smoker or not, where 0 represents a non-smoker and 1 represents a smoker.\",\n",
      "  \"charges\": \"The total insurance charges or costs incurred by the insured person.\"\n",
      "}\n",
      "\n",
      "======================================================================\n",
      "âœ… Schema analysis completed for insurance\n",
      "\n",
      "ğŸ“‹ CLAUDE-GENERATED SCHEMA FOR INSURANCE\n",
      "======================================================================\n",
      "\n",
      "ğŸ—‚ï¸ DDL (Data Definition Language):\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "CREATE TABLE insurance (\n",
      "  `age` INT NOT NULL,\n",
      "  `sex` BOOLEAN NOT NULL,\n",
      "  `bmi` DECIMAL(5,2) NOT NULL,\n",
      "  `children` INT NOT NULL,\n",
      "  `smoker` BOOLEAN NOT NULL,\n",
      "  `charges` DECIMAL(10,5) NOT NULL\n",
      ");\n",
      "\n",
      "ğŸ“– Column Descriptions:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "{\n",
      "  \"age\": \"The age of the insured person in years.\",\n",
      "  \"sex\": \"The biological sex of the insured person, where 0 represents female and 1 represents male.\",\n",
      "  \"bmi\": \"The body mass index (BMI) of the insured person, calculated as weight in kilograms divided by the square of height in meters.\",\n",
      "  \"children\": \"The number of children the insured person has.\",\n",
      "  \"smoker\": \"Indicates whether the insured person is a smoker or not, where 0 represents a non-smoker and 1 represents a smoker.\",\n",
      "  \"charges\": \"The total insurance charges or costs incurred by the insured person.\"\n",
      "}\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test the corrected schema generation functionality\n",
    "print(\"ğŸ§ª Testing Schema Generation...\")\n",
    "\n",
    "# Test Claude connection first\n",
    "connection_ok = test_claude_connection()\n",
    "\n",
    "if connection_ok:\n",
    "    # Test with the insurance dataset (smaller dataset for testing)\n",
    "    if 'insurance' in datasets:\n",
    "        print(f\"\\nğŸ” Testing schema generation with 'insurance' dataset...\")\n",
    "        ddl, descriptions = analyze_schema_with_claude(datasets['insurance'], 'insurance', sample_size=3)\n",
    "        display_schema_results(ddl, descriptions, 'insurance')\n",
    "    else:\n",
    "        print(\"âš ï¸ Insurance dataset not found in loaded datasets.\")\n",
    "else:\n",
    "    print(\"âŒ Cannot test schema generation without Claude API connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b1140cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing extract_column_samples function...\n",
      "âœ… Function works! Extracted 6 column samples:\n",
      "  age: [57, 41, 56]\n",
      "  sex: [0, 0, 0]\n",
      "  bmi: [38.06, 25.555, 26.73]\n",
      "  children: [2, 2, 0]\n",
      "  smoker: [0, 1, 0]\n",
      "  charges: [6082.405, 3206.49135, 5138.2567]\n"
     ]
    }
   ],
   "source": [
    "# Simple test of the corrected extract_column_samples function\n",
    "print(\"Testing extract_column_samples function...\")\n",
    "\n",
    "if 'insurance' in datasets:\n",
    "    # Test the function that was causing the error\n",
    "    samples = extract_column_samples(datasets['insurance'], sample_size=3)\n",
    "    print(f\"âœ… Function works! Extracted {len(samples)} column samples:\")\n",
    "    for col, sample_values in samples.items():\n",
    "        print(f\"  {col}: {sample_values}\")\n",
    "else:\n",
    "    print(\"âŒ Insurance dataset not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf565b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing Schema Generation with SAP Dataset...\n",
      "============================================================\n",
      "âœ… Claude API connection successful\n",
      "\n",
      "ğŸ” Testing schema generation with 'sap' dataset...\n",
      "ğŸ“Š SAP Dataset Info:\n",
      "   Rows: 100,000\n",
      "   Columns: 64\n",
      "   Column names: ['VBELN', 'GPAG', 'JPARVWGPAG', 'ANGDT', 'BNDDT', 'VBTYP', 'TRVOG', 'AUART', 'AUARTGRP', 'WAERK', 'NETWR', 'VKORG', 'VTWEG', 'SPART', 'VKGRP', 'VKBUR', 'EXEMPLART', 'GSBER', 'GSKST', 'KNUMV', 'FAKSP', 'KALSM', 'KURST', 'BSTNK', 'BSARK', 'BSTDK', 'BSTZD', 'IHREZ', 'STAFO', 'STWAE', 'XIMMATRIK', 'IMMATDAT', 'XSEPFKKOPF', 'XREMRECHT', 'REMSP', 'KORRGRD', 'REMVON', 'REMBIS', 'REFBELEG', 'REMDATUM', 'REMSCHEIN', 'IVWDATUM', 'POSNR_LAST', 'POSEX_LAST', 'KPOSN_LAST', 'XFKBASAUFT', 'XFKBASLIEF', 'XJKSOFAKT', 'ERFUSER', 'ERFDATE', 'ERFTIME', 'AENUSER', 'AENDATE', 'AENTIME', 'XWBZABO', 'XFKVDICHT', 'XSTATARC', 'XNOMESS', 'KALSM_AMO', 'XRENEWAL', 'AMORTN', 'REKLERGB', 'REKLTYP', 'REKLDATUM']\n",
      "\n",
      "ğŸ”¬ Testing extract_column_samples function...\n",
      "âœ… Successfully extracted samples from 64 columns:\n",
      "   VBELN: [28242, 21971, 148725]\n",
      "   GPAG: [93175844, 93155647, 93150788]\n",
      "   JPARVWGPAG: [1, 1, 1]\n",
      "   ANGDT: [0, 0, 0]\n",
      "   BNDDT: [0, 0, 0]\n",
      "   VBTYP: ['C', 'C', 'C']\n",
      "   TRVOG: [0, 0, 0]\n",
      "   AUART: ['ABO', 'ABO', 'ABO']\n",
      "   AUARTGRP: [1, 1, 1]\n",
      "   WAERK: ['EUR', 'EUR', 'EUR']\n",
      "   NETWR: [115.47, 60.79, 20.26]\n",
      "   VKORG: [210, 210, 210]\n",
      "   VTWEG: ['AB', 'AB', 'AB']\n",
      "   SPART: ['ZT', 'ZT', 'ZT']\n",
      "   VKGRP: ['NULL']\n",
      "   VKBUR: ['NULL']\n",
      "   EXEMPLART: ['0', '0', '0']\n",
      "   GSBER: ['NULL']\n",
      "   GSKST: ['NULL']\n",
      "   KNUMV: [181916, 182263, 135212]\n",
      "   FAKSP: ['EV', 'EV', 'EV']\n",
      "   KALSM: ['ZSTDA1', 'ZSTDA1', 'ZSTDA1']\n",
      "   KURST: ['EURO', 'EURO', 'EURO']\n",
      "   BSTNK: ['4530307054', 'K-Nr. 6000580', '8282576.1']\n",
      "   BSARK: ['P', 'T', 'T']\n",
      "   BSTDK: [0, 0, 0]\n",
      "   BSTZD: ['NULL']\n",
      "   IHREZ: ['fau', 'fau', 'fau']\n",
      "   STAFO: ['NULL']\n",
      "   STWAE: ['NULL']\n",
      "   XIMMATRIK: ['X', 'X', 'X']\n",
      "   IMMATDAT: [0, 0, 0]\n",
      "   XSEPFKKOPF: ['X', 'X', 'X']\n",
      "   XREMRECHT: ['X', 'X', 'X']\n",
      "   REMSP: ['NULL']\n",
      "   KORRGRD: ['NULL']\n",
      "   REMVON: [0, 0, 0]\n",
      "   REMBIS: [0, 0, 0]\n",
      "   REFBELEG: ['0142391/0000/0036928', '0144789/0000/0038729', '0147413/0000/0040634']\n",
      "   REMDATUM: [0, 0, 0]\n",
      "   REMSCHEIN: [0, 0, 0]\n",
      "   IVWDATUM: [0, 0, 0]\n",
      "   POSNR_LAST: [1, 2, 36]\n",
      "   POSEX_LAST: [10, 10, 10]\n",
      "   KPOSN_LAST: [9, 44, 9]\n",
      "   XFKBASAUFT: ['X', 'X', 'X']\n",
      "   XFKBASLIEF: ['X', 'X', 'X']\n",
      "   XJKSOFAKT: ['X']\n",
      "   ERFUSER: ['ZIMMERMANN', 'ZIMMERMANN', 'ZIMMERMANN']\n",
      "   ERFDATE: [20010519, 20010519, 20010520]\n",
      "   ERFTIME: [182412, 33446, 151200]\n",
      "   AENUSER: ['OSCRFC', 'FABERH', 'ALLGAIERE']\n",
      "   AENDATE: [20170109, 20030808, 20201118]\n",
      "   AENTIME: [125257, 121446, 101144]\n",
      "   XWBZABO: ['NULL']\n",
      "   XFKVDICHT: ['NULL']\n",
      "   XSTATARC: ['NULL']\n",
      "   XNOMESS: ['NULL']\n",
      "   KALSM_AMO: ['NULL']\n",
      "   XRENEWAL: ['NULL']\n",
      "   AMORTN: ['NULL']\n",
      "   REKLERGB: ['04', '04', '04']\n",
      "   REKLTYP: [0, 0, 0]\n",
      "   REKLDATUM: [0, 0, 0]\n",
      "\n",
      "ğŸ¤– Proceeding with full schema generation...\n",
      "ğŸ” Analyzing schema for sap using Claude...\n",
      "âœ”ï¸ Extracted samples from 64 columns\n",
      "ğŸ¤– Calling Claude API for schema analysis...\n",
      "âœ… Schema analysis completed for sap\n",
      "\n",
      "ğŸ“‹ CLAUDE-GENERATED SCHEMA FOR SAP\n",
      "======================================================================\n",
      "\n",
      "ğŸ—‚ï¸ DDL (Data Definition Language):\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "CREATE TABLE sap (\n",
      "  `VBELN` INT NOT NULL,\n",
      "  `GPAG` INT NOT NULL,\n",
      "  `JPARVWGPAG` BOOLEAN NOT NULL,\n",
      "  `ANGDT` INT NOT NULL,\n",
      "  `BNDDT` INT NOT NULL,\n",
      "  `VBTYP` VARCHAR(1) NOT NULL,\n",
      "  `TRVOG` INT NOT NULL,\n",
      "  `AUART` VARCHAR(3) NOT NULL,\n",
      "  `AUARTGRP` INT NOT NULL,\n",
      "  `WAERK` VARCHAR(3) NOT NULL,\n",
      "  `NETWR` DECIMAL(15,2) NOT NULL,\n",
      "  `VKORG` INT NOT NULL,\n",
      "  `VTWEG` VARCHAR(2) NOT NULL,\n",
      "  `SPART` VARCHAR(2) NOT NULL,\n",
      "  `VKGRP` VARCHAR(3),\n",
      "  `VKBUR` VARCHAR(4),\n",
      "  `EXEMPLART` VARCHAR(1) NOT NULL,\n",
      "  `GSBER` VARCHAR(4),\n",
      "  `GSKST` VARCHAR(8),\n",
      "  `KNUMV` INT NOT NULL,\n",
      "  `FAKSP` VARCHAR(2) NOT NULL,\n",
      "  `KALSM` VARCHAR(6) NOT NULL,\n",
      "  `KURST` VARCHAR(5) NOT NULL,\n",
      "  `BSTNK` VARCHAR(20) NOT NULL,\n",
      "  `BSARK` VARCHAR(1) NOT NULL,\n",
      "  `BSTDK` INT NOT NULL,\n",
      "  `BSTZD` VARCHAR(4),\n",
      "  `IHREZ` VARCHAR(20) NOT NULL,\n",
      "  `STAFO` VARCHAR(18),\n",
      "  `STWAE` VARCHAR(18),\n",
      "  `XIMMATRIK` VARCHAR(1) NOT NULL,\n",
      "  `IMMATDAT` INT NOT NULL,\n",
      "  `XSEPFKKOPF` VARCHAR(1) NOT NULL,\n",
      "  `XREMRECHT` VARCHAR(1) NOT NULL,\n",
      "  `REMSP` VARCHAR(2),\n",
      "  `KORRGRD` VARCHAR(2),\n",
      "  `REMVON` INT NOT NULL,\n",
      "  `REMBIS` INT NOT NULL,\n",
      "  `REFBELEG` VARCHAR(20) NOT NULL,\n",
      "  `REMDATUM` INT NOT NULL,\n",
      "  `REMSCHEIN` INT NOT NULL,\n",
      "  `IVWDATUM` INT NOT NULL,\n",
      "  `POSNR_LAST` INT NOT NULL,\n",
      "  `POSEX_LAST` INT NOT NULL,\n",
      "  `KPOSN_LAST` INT NOT NULL,\n",
      "  `XFKBASAUFT` VARCHAR(1) NOT NULL,\n",
      "  `XFKBASLIEF` VARCHAR(1) NOT NULL,\n",
      "  `XJKSOFAKT` VARCHAR(1),\n",
      "  `ERFUSER` VARCHAR(12) NOT NULL,\n",
      "  `ERFDATE` INT NOT NULL,\n",
      "  `ERFTIME` INT NOT NULL,\n",
      "  `AENUSER` VARCHAR(12) NOT NULL,\n",
      "  `AENDATE` INT NOT NULL,\n",
      "  `AENTIME` INT NOT NULL,\n",
      "  `XWBZABO` VARCHAR(1),\n",
      "  `XFKVDICHT` VARCHAR(1),\n",
      "  `XSTATARC` VARCHAR(1),\n",
      "  `XNOMESS` VARCHAR(1),\n",
      "  `KALSM_AMO` VARCHAR(6),\n",
      "  `XRENEWAL` VARCHAR(1),\n",
      "  `AMORTN` VARCHAR(2),\n",
      "  `REKLERGB` VARCHAR(2) NOT NULL,\n",
      "  `REKLTYP` INT NOT NULL,\n",
      "  `REKLDATUM` INT NOT NULL\n",
      ");\n",
      "\n",
      "ğŸ“– Column Descriptions:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "{\n",
      "  \"VBELN\": \"Sales document number, a unique identifier for the sales order or billing document.\",\n",
      "  \"GPAG\": \"Purchasing group, representing a group of materials or services that are procured together.\",\n",
      "  \"JPARVWGPAG\": \"Flag indicating whether the purchasing group is relevant for parallel processing.\",\n",
      "  \"ANGDT\": \"Creation date of the sales document, stored as an integer value representing the date.\",\n",
      "  \"BNDDT\": \"Binding date for the sales document, stored as an integer value representing the date.\",\n",
      "  \"VBTYP\": \"Sales document category, a single character code representing the type of sales document.\",\n",
      "  \"TRVOG\": \"Pricing procedure, an integer code representing the method used for pricing the sales document.\",\n",
      "  \"AUART\": \"Sales document type, a three-character code representing the specific type of sales document.\",\n",
      "  \"AUARTGRP\": \"Sales document type group, an integer code representing a group of related sales document types.\",\n",
      "  \"WAERK\": \"Currency code, a three-letter ISO code representing the currency used for the sales document.\",\n",
      "  \"NETWR\": \"Net value of the sales document, the total amount excluding taxes and other charges.\",\n",
      "  \"VKORG\": \"Sales organization, an integer code representing the organizational unit responsible for sales activities.\",\n",
      "  \"VTWEG\": \"Distribution channel, a two-character code representing the channel through which the sales document is processed.\",\n",
      "  \"SPART\": \"Product group, a two-character code representing a group of related products or services.\",\n",
      "  \"VKGRP\": \"Sales group, a three-character code representing a group of customers or sales territories.\",\n",
      "  \"VKBUR\": \"Sales office, a four-character code representing the office or location responsible for the sales activities.\",\n",
      "  \"EXEMPLART\": \"Exemplar type, a single-character code representing the type of exemplar or sample associated with the sales document.\",\n",
      "  \"GSBER\": \"Business area, a four-character code representing the business area or division associated with the sales document.\",\n",
      "  \"GSKST\": \"Cost center, an eight-character code representing the cost center associated with the sales document.\",\n",
      "  \"KNUMV\": \"Customer number, a unique identifier for the customer associated with the sales document.\",\n",
      "  \"FAKSP\": \"Billing block, a two-character code indicating whether billing is allowed or blocked for the sales document.\",\n",
      "  \"KALSM\": \"Condition type, a six-character code representing the type of condition or pricing rule applied to the sales document.\",\n",
      "  \"KURST\": \"Condition currency, a five-character code representing the currency used for the condition or pricing rule.\",\n",
      "  \"BSTNK\": \"Customer reference, a reference or identifier provided by the customer for the sales document.\",\n",
      "  \"BSARK\": \"Customer reference type, a single-character code representing the type of customer reference provided.\",\n",
      "  \"BSTDK\": \"Customer reference date, an integer value representing the date associated with the customer reference.\",\n",
      "  \"BSTZD\": \"Customer reference time, a four-character code representing the time associated with the customer reference.\",\n",
      "  \"IHREZ\": \"Person responsible, the name or identifier of the person responsible for the sales document.\",\n",
      "  \"STAFO\": \"Statistical material group, an eighteen-character code representing a statistical grouping of materials or products.\",\n",
      "  \"STWAE\": \"Statistical currency, an eighteen-character code representing a statistical currency used for reporting or analysis purposes.\",\n",
      "  \"XIMMATRIK\": \"Flag indicating whether the sales document is subject to immaterial processing.\",\n",
      "  \"IMMATDAT\": \"Immaterial date, an integer value representing the date associated with immaterial processing.\",\n",
      "  \"XSEPFKKOPF\": \"Flag indicating whether the sales document is subject to separate invoice processing.\",\n",
      "  \"XREMRECHT\": \"Flag indicating whether the sales document is subject to remuneration rights processing.\",\n",
      "  \"REMSP\": \"Remuneration type, a two-character code representing the type of remuneration associated with the sales document.\",\n",
      "  \"KORRGRD\": \"Correction reason, a two-character code representing the reason for any corrections or adjustments made to the sales document.\",\n",
      "  \"REMVON\": \"Remuneration start date, an integer value representing the start date for remuneration processing.\",\n",
      "  \"REMBIS\": \"Remuneration end date, an integer value representing the end date for remuneration processing.\",\n",
      "  \"REFBELEG\": \"Reference document, a reference or identifier for a related document associated with the sales document.\",\n",
      "  \"REMDATUM\": \"Remuneration date, an integer value representing the date associated with remuneration processing.\",\n",
      "  \"REMSCHEIN\": \"Remuneration certificate, an integer value representing a certificate or identifier associated with remuneration processing.\",\n",
      "  \"IVWDATUM\": \"Inward processing date, an integer value representing the date associated with inward processing activities.\",\n",
      "  \"POSNR_LAST\": \"Last item number, an integer value representing the item or line number of the last item in the sales document.\",\n",
      "  \"POSEX_LAST\": \"Last item position, an integer value representing the position or sequence of the last item in the sales document.\",\n",
      "  \"KPOSN_LAST\": \"Last condition item, an integer value representing the condition or pricing item associated with the last item in the sales document.\",\n",
      "  \"XFKBASAUFT\": \"Flag indicating whether the sales document is subject to order-based pricing.\",\n",
      "  \"XFKBASLIEF\": \"Flag indicating whether the sales document is subject to delivery-based pricing.\",\n",
      "  \"XJKSOFAKT\": \"Flag indicating whether the sales document is subject to immediate invoicing.\",\n",
      "  \"ERFUSER\": \"User who created the sales document, the username or identifier of the person who initially created the record.\",\n",
      "  \"ERFDATE\": \"Creation date, an integer value representing the date when the sales document was created.\",\n",
      "  \"ERFTIME\": \"Creation time, an integer value representing the time when the sales document was created.\",\n",
      "  \"AENUSER\": \"User who last modified the sales document, the username or identifier of the person who made the most recent changes.\",\n",
      "  \"AENDATE\": \"Last modification date, an integer value representing the date when the sales document was last modified.\",\n",
      "  \"AENTIME\": \"Last modification time, an integer value representing the time when the sales document was last modified.\",\n",
      "  \"XWBZABO\": \"Flag indicating whether the sales document is subject to a specific processing rule or condition.\",\n",
      "  \"XFKVDICHT\": \"Flag indicating whether the sales document is subject to a specific pricing condition or rule.\",\n",
      "  \"XSTATARC\": \"Flag indicating whether the sales document is subject to statistical archiving or reporting.\",\n",
      "  \"XNOMESS\": \"Flag indicating whether the sales document is subject to a specific measurement or calculation rule.\",\n",
      "  \"KALSM_AMO\": \"Condition type for amortization, a six-character code representing the type of condition or pricing rule applied for amortization purposes.\",\n",
      "  \"XRENEWAL\": \"Flag indicating whether the sales document is subject to renewal or extension processing.\",\n",
      "  \"AMORTN\": \"Amortization code, a two-character code representing the amortization method or rule applied to the sales document.\",\n",
      "  \"REKLERGB\": \"Complaint result, a two-character code representing the outcome or result of a complaint or issue related to the sales document.\",\n",
      "  \"REKLTYP\": \"Complaint type, an integer code representing the type or category of complaint associated with the sales document.\",\n",
      "  \"REKLDATUM\": \"Complaint date, an integer value representing the date when a complaint or issue was reported for the sales document.\"\n",
      "}\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test schema generation with SAP dataset\n",
    "print(\"ğŸ§ª Testing Schema Generation with SAP Dataset...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test Claude connection first\n",
    "connection_ok = test_claude_connection()\n",
    "\n",
    "if connection_ok:\n",
    "    # Test with the SAP dataset\n",
    "    if 'sap' in datasets:\n",
    "        print(f\"\\nğŸ” Testing schema generation with 'sap' dataset...\")\n",
    "        \n",
    "        # First show basic info about the dataset\n",
    "        sap_df = datasets['sap']\n",
    "        print(f\"ğŸ“Š SAP Dataset Info:\")\n",
    "        print(f\"   Rows: {sap_df.shape[0]:,}\")\n",
    "        print(f\"   Columns: {sap_df.shape[1]}\")\n",
    "        print(f\"   Column names: {list(sap_df.columns)}\")\n",
    "        \n",
    "        # Test the extract_column_samples function\n",
    "        print(\"\\nğŸ”¬ Testing extract_column_samples function...\")\n",
    "        try:\n",
    "            samples = extract_column_samples(sap_df, sample_size=3)\n",
    "            print(f\"âœ… Successfully extracted samples from {len(samples)} columns:\")\n",
    "            for col, sample_values in samples.items():\n",
    "                print(f\"   {col}: {sample_values}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error in extract_column_samples: {e}\")\n",
    "            print(\"Cannot proceed with schema generation.\")\n",
    "        else:\n",
    "            # If sample extraction worked, proceed with full schema generation\n",
    "            print(\"\\nğŸ¤– Proceeding with full schema generation...\")\n",
    "            try:\n",
    "                ddl, descriptions = analyze_schema_with_claude(sap_df, 'sap', sample_size=3)\n",
    "                display_schema_results(ddl, descriptions, 'sap')\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error in schema generation: {e}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ SAP dataset not found in loaded datasets.\")\n",
    "        print(f\"Available datasets: {list(datasets.keys()) if 'datasets' in globals() else 'None'}\")\n",
    "else:\n",
    "    print(\"âŒ Cannot test schema generation without Claude API connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9126ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” COMPREHENSIVE SAP DATASET SCHEMA GENERATION TEST\n",
      "======================================================================\n",
      "\n",
      "ğŸ”— Testing Claude API connection...\n",
      "âœ… Claude API connection successful\n",
      "\n",
      "ğŸ¯ TESTING SAP DATASET SCHEMA GENERATION\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“Š SAP Dataset Overview:\n",
      "   ğŸ“ˆ Rows: 100,000\n",
      "   ğŸ“‹ Columns: 64\n",
      "   ğŸ’¾ Memory: 25.5 MB\n",
      "   ğŸ·ï¸ Column names: ['VBELN', 'GPAG', 'JPARVWGPAG', 'ANGDT', 'BNDDT', 'VBTYP', 'TRVOG', 'AUART', 'AUARTGRP', 'WAERK', 'NETWR', 'VKORG', 'VTWEG', 'SPART', 'VKGRP', 'VKBUR', 'EXEMPLART', 'GSBER', 'GSKST', 'KNUMV', 'FAKSP', 'KALSM', 'KURST', 'BSTNK', 'BSARK', 'BSTDK', 'BSTZD', 'IHREZ', 'STAFO', 'STWAE', 'XIMMATRIK', 'IMMATDAT', 'XSEPFKKOPF', 'XREMRECHT', 'REMSP', 'KORRGRD', 'REMVON', 'REMBIS', 'REFBELEG', 'REMDATUM', 'REMSCHEIN', 'IVWDATUM', 'POSNR_LAST', 'POSEX_LAST', 'KPOSN_LAST', 'XFKBASAUFT', 'XFKBASLIEF', 'XJKSOFAKT', 'ERFUSER', 'ERFDATE', 'ERFTIME', 'AENUSER', 'AENDATE', 'AENTIME', 'XWBZABO', 'XFKVDICHT', 'XSTATARC', 'XNOMESS', 'KALSM_AMO', 'XRENEWAL', 'AMORTN', 'REKLERGB', 'REKLTYP', 'REKLDATUM']\n",
      "\n",
      "ğŸ”¬ Step 1: Testing column sampling...\n",
      "âœ… Successfully extracted samples from 64 columns:\n",
      "   ğŸ“ VBELN: [63297, 124643, 66103, 47867, 296477]\n",
      "   ğŸ“ GPAG: [93176325, 93130954, 93135491, 93367420, 93109064]\n",
      "   ğŸ“ JPARVWGPAG: [1, 1, 1, 1, 1]\n",
      "   ğŸ“ ANGDT: [0, 0, 0, 0, 0]\n",
      "   ğŸ“ BNDDT: [0, 0, 0, 0, 0]\n",
      "   ğŸ“ VBTYP: ['C', 'C', 'C', 'C', 'C']\n",
      "   ğŸ“ TRVOG: [0, 0, 0, 0, 0]\n",
      "   ğŸ“ AUART: ['ABO', 'ABO', 'ABO', 'ABO', 'ABO']\n",
      "   ğŸ“ AUARTGRP: [1, 1, 1, 1, 1]\n",
      "   ğŸ“ WAERK: ['EUR', 'EUR', 'EUR', 'EUR', 'EUR']\n",
      "   ğŸ“ NETWR: [20.26, 52.9, 56.05, 106.49, 54.55]\n",
      "   ğŸ“ VKORG: [210, 210, 210, 210, 210]\n",
      "   ğŸ“ VTWEG: ['AB', 'AB', 'AB', 'AB', 'AB']\n",
      "   ğŸ“ SPART: ['ZT', 'ZT', 'ZT', 'ZT', 'ZT']\n",
      "   ğŸ“ VKGRP: ['NULL']\n",
      "   ğŸ“ VKBUR: ['NULL']\n",
      "   ğŸ“ EXEMPLART: ['0', '0', '0', '0', '0']\n",
      "   ğŸ“ GSBER: ['NULL']\n",
      "   ğŸ“ GSKST: ['NULL']\n",
      "   ğŸ“ KNUMV: [215204, 128268, 210441, 127846, 225087]\n",
      "   ğŸ“ FAKSP: ['EV', 'EV', 'EV', 'EV', 'EV']\n",
      "   ğŸ“ KALSM: ['ZSTDA1', 'ZSTDA1', 'ZSTDA1', 'ZSTDA1', 'ZSTDA1']\n",
      "   ğŸ“ KURST: ['EURO', 'EURO', 'EURO', 'EURO', 'EURO']\n",
      "   ğŸ“ BSTNK: ['8282576.1', 'K-Nr. 6000580', '8284601.1', '200293-0001/14405002', '10132201416']\n",
      "   ğŸ“ BSARK: ['P', 'T', 'T', 'T', 'P']\n",
      "   ğŸ“ BSTDK: [0, 0, 0, 0, 0]\n",
      "   ğŸ“ BSTZD: ['NULL']\n",
      "   ğŸ“ IHREZ: ['fau', 'fau', 'fau', 'fau', 'fau']\n",
      "   ğŸ“ STAFO: ['NULL']\n",
      "   ğŸ“ STWAE: ['NULL']\n",
      "   ğŸ“ XIMMATRIK: ['X', 'X', 'X', 'X', 'X']\n",
      "   ğŸ“ IMMATDAT: [0, 0, 0, 0, 0]\n",
      "   ğŸ“ XSEPFKKOPF: ['X', 'X', 'X']\n",
      "   ğŸ“ XREMRECHT: ['X', 'X', 'X', 'X', 'X']\n",
      "   ğŸ“ REMSP: ['NULL']\n",
      "   ğŸ“ KORRGRD: ['NULL']\n",
      "   ğŸ“ REMVON: [0, 0, 0, 0, 0]\n",
      "   ğŸ“ REMBIS: [0, 0, 0, 0, 0]\n",
      "   ğŸ“ REFBELEG: ['4096514/0000/1871584', '0383308/0000/0163728', '0171359/0000/0059018', '0352725/0000/0148634', '4197021/0000/1829107']\n",
      "   ğŸ“ REMDATUM: [0, 0, 0, 0, 0]\n",
      "   ğŸ“ REMSCHEIN: [0, 0, 0, 0, 0]\n",
      "   ğŸ“ IVWDATUM: [0, 0, 0, 0, 0]\n",
      "   ğŸ“ POSNR_LAST: [3, 1, 1, 3, 1]\n",
      "   ğŸ“ POSEX_LAST: [10, 10, 10, 10, 10]\n",
      "   ğŸ“ KPOSN_LAST: [1, 5, 1, 1, 7]\n",
      "   ğŸ“ XFKBASAUFT: ['X', 'X', 'X', 'X', 'X']\n",
      "   ğŸ“ XFKBASLIEF: ['X', 'X', 'X', 'X', 'X']\n",
      "   ğŸ“ XJKSOFAKT: ['X']\n",
      "   ğŸ“ ERFUSER: ['ZIMMERMANN', 'ZIMMERMANN', 'ZIMMERMANN', 'ZIMMERMANN', 'ZIMMERMANN']\n",
      "   ğŸ“ ERFDATE: [20010520, 20010519, 20010519, 20010520, 20010519]\n",
      "   ğŸ“ ERFTIME: [114514, 24935, 173449, 141904, 161959]\n",
      "   ğŸ“ AENUSER: ['HAGNER', 'FEURER', 'FEURER', 'POSSELT', 'BECKER']\n",
      "   ğŸ“ AENDATE: [20011231, 20200818, 20011231, 20011231, 20011231]\n",
      "   ğŸ“ AENTIME: [0, 113634, 130117, 170821, 143646]\n",
      "   ğŸ“ XWBZABO: ['NULL']\n",
      "   ğŸ“ XFKVDICHT: ['NULL']\n",
      "   ğŸ“ XSTATARC: ['NULL']\n",
      "   ğŸ“ XNOMESS: ['NULL']\n",
      "   ğŸ“ KALSM_AMO: ['NULL']\n",
      "   ğŸ“ XRENEWAL: ['NULL']\n",
      "   ğŸ“ AMORTN: ['NULL']\n",
      "   ğŸ“ REKLERGB: ['04', '04', '04', '04', '04']\n",
      "   ğŸ“ REKLTYP: [0, 0, 0, 0, 0]\n",
      "   ğŸ“ REKLDATUM: [0, 0, 0, 0, 0]\n",
      "\n",
      "ğŸ¤– Step 2: Generating schema with Claude API...\n",
      "ğŸ” Analyzing schema for sap_data using Claude...\n",
      "âœ”ï¸ Extracted samples from 64 columns\n",
      "ğŸ¤– Calling Claude API for schema analysis...\n",
      "âŒ Failed to parse Claude response as JSON: Expecting ',' delimiter: line 202 column 6 (char 6742)\n",
      "Raw response: ```json\n",
      "{\n",
      "  \"columns\": {\n",
      "    \"VBELN\": {\n",
      "      \"sql_type\": \"INT\",\n",
      "      \"nullable\": false,\n",
      "      \"description\": \"The sales document number, a unique identifier for the sales order or billing document.\"\n",
      "    },\n",
      "    \"GPAG\": {\n",
      "      \"sql_type\": \"INT\",\n",
      "      \"nullable\": false,\n",
      "      \"description\": \"The internal SAP document number associated with the sales document.\"\n",
      "    },\n",
      "    \"JPARVWGPAG\": {\n",
      "      \"sql_type\": \"BOOLEAN\",\n",
      "      \"nullable\": false,\n",
      "      \"description\": \"A flag indicating whether the doc...\n",
      "\n",
      "================================================================================\n",
      "ğŸ‰ SAP DATASET SCHEMA GENERATION COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ CLAUDE-GENERATED SCHEMA FOR SAP_DATA\n",
      "======================================================================\n",
      "\n",
      "ğŸ—‚ï¸ DDL (Data Definition Language):\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "-- Error generating schema\n",
      "\n",
      "ğŸ“– Column Descriptions:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "{}\n",
      "\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ˆ ADDITIONAL SCHEMA INSIGHTS:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âœ… Generated descriptions for 0 columns\n",
      "âœ… DDL contains 0 field definitions\n",
      "âœ… Schema ready for database implementation\n",
      "\n",
      "ğŸ SAP Dataset Schema Generation Test Complete\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive SAP Dataset Schema Generation Test\n",
    "print(\"ğŸ” COMPREHENSIVE SAP DATASET SCHEMA GENERATION TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Step 1: Check if datasets are loaded\n",
    "if 'datasets' not in globals() or not datasets:\n",
    "    print(\"âš ï¸ No datasets found. Loading datasets first...\")\n",
    "    \n",
    "    # Load datasets using the existing function\n",
    "    file_paths = [\n",
    "        \"/Users/tomasnagy/Scavenger AI/insurance.csv\",\n",
    "        \"/Users/tomasnagy/Scavenger AI/sap.csv\"\n",
    "    ]\n",
    "    \n",
    "    datasets = {}\n",
    "    start = time.time()\n",
    "    \n",
    "    for path in file_paths:\n",
    "        name = Path(path).stem\n",
    "        print(f\"\\nğŸ“‚ Loading: {name.upper()}\")\n",
    "        \n",
    "        # Use the simple processing function\n",
    "        df = process_csv_file_simple(path)\n",
    "        \n",
    "        if df is not None:\n",
    "            datasets[name] = df\n",
    "            print(f\"âœ… {name} loaded successfully: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "        else:\n",
    "            print(f\"âŒ Failed to load {name}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Dataset loading completed in {format_duration(time.time() - start)}\")\n",
    "\n",
    "# Step 2: Test Claude API connection\n",
    "print(\"\\nğŸ”— Testing Claude API connection...\")\n",
    "connection_ok = test_claude_connection()\n",
    "\n",
    "# Step 3: Test SAP dataset specifically\n",
    "if connection_ok and 'sap' in datasets:\n",
    "    print(\"\\nğŸ¯ TESTING SAP DATASET SCHEMA GENERATION\")\n",
    "    print(\"â”€\" * 50)\n",
    "    \n",
    "    sap_df = datasets['sap']\n",
    "    \n",
    "    # Show dataset info\n",
    "    print(f\"ğŸ“Š SAP Dataset Overview:\")\n",
    "    print(f\"   ğŸ“ˆ Rows: {sap_df.shape[0]:,}\")\n",
    "    print(f\"   ğŸ“‹ Columns: {sap_df.shape[1]}\")\n",
    "    print(f\"   ğŸ’¾ Memory: {sap_df.estimated_size('mb'):.1f} MB\")\n",
    "    print(f\"   ğŸ·ï¸ Column names: {list(sap_df.columns)}\")\n",
    "    \n",
    "    # Test column sampling first\n",
    "    print(\"\\nğŸ”¬ Step 1: Testing column sampling...\")\n",
    "    try:\n",
    "        samples = extract_column_samples(sap_df, sample_size=5)\n",
    "        print(f\"âœ… Successfully extracted samples from {len(samples)} columns:\")\n",
    "        for col, sample_values in samples.items():\n",
    "            print(f\"   ğŸ“ {col}: {sample_values}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in column sampling: {e}\")\n",
    "        print(\"Cannot proceed with schema generation.\")\n",
    "    else:\n",
    "        # Proceed with full schema generation\n",
    "        print(\"\\nğŸ¤– Step 2: Generating schema with Claude API...\")\n",
    "        try:\n",
    "            ddl, descriptions = analyze_schema_with_claude(sap_df, 'sap_data', sample_size=5)\n",
    "            \n",
    "            # Display results with enhanced formatting\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"ğŸ‰ SAP DATASET SCHEMA GENERATION COMPLETED SUCCESSFULLY!\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            display_schema_results(ddl, descriptions, 'sap_data')\n",
    "            \n",
    "            # Additional analysis\n",
    "            print(\"\\nğŸ“ˆ ADDITIONAL SCHEMA INSIGHTS:\")\n",
    "            print(\"â”€\" * 40)\n",
    "            \n",
    "            # Parse the descriptions to count field types\n",
    "            try:\n",
    "                desc_data = json.loads(descriptions)\n",
    "                print(f\"âœ… Generated descriptions for {len(desc_data)} columns\")\n",
    "                print(f\"âœ… DDL contains {ddl.count('`')} field definitions\")\n",
    "                print(f\"âœ… Schema ready for database implementation\")\n",
    "            except:\n",
    "                print(\"âœ… Schema generation completed (descriptions in text format)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error in schema generation: {e}\")\n",
    "            import traceback\n",
    "            print(f\"Full error: {traceback.format_exc()}\")\n",
    "            \n",
    "else:\n",
    "    if not connection_ok:\n",
    "        print(\"âŒ Cannot test SAP schema generation - Claude API connection failed\")\n",
    "    elif 'sap' not in datasets:\n",
    "        print(f\"âŒ SAP dataset not found. Available datasets: {list(datasets.keys()) if 'datasets' in globals() else 'None'}\")\n",
    "        print(\"Make sure the SAP CSV file exists at: /Users/tomasnagy/Scavenger AI/sap.csv\")\n",
    "    else:\n",
    "        print(\"âŒ Unknown error - cannot proceed with SAP testing\")\n",
    "\n",
    "print(\"\\nğŸ SAP Dataset Schema Generation Test Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ccf56224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ IMPROVED SAP DATASET SCHEMA GENERATION\n",
      "======================================================================\n",
      "\n",
      "ğŸ¯ Testing improved SAP dataset schema generation...\n",
      "âœ… Claude API connection successful\n",
      "\n",
      "ğŸ“ˆ SAP Dataset: 100,000 rows Ã— 64 columns\n",
      "ğŸ“Š Dataset: sap_sales_data with 64 columns\n",
      "âš ï¸ Large dataset detected (64 columns). Processing in batches...\n",
      "\n",
      "ğŸ”„ Processing batch 1: columns 1-20\n",
      "ğŸ¤– Calling Claude API for batch 1...\n",
      "âœ… Batch 1 completed successfully\n",
      "\n",
      "ğŸ”„ Processing batch 2: columns 21-40\n",
      "ğŸ¤– Calling Claude API for batch 2...\n",
      "âœ… Batch 2 completed successfully\n",
      "\n",
      "ğŸ”„ Processing batch 3: columns 41-60\n",
      "ğŸ¤– Calling Claude API for batch 3...\n",
      "âœ… Batch 3 completed successfully\n",
      "\n",
      "ğŸ”„ Processing batch 4: columns 61-64\n",
      "ğŸ¤– Calling Claude API for batch 4...\n",
      "âœ… Batch 4 completed successfully\n",
      "\n",
      "================================================================================\n",
      "ğŸ‰ IMPROVED SAP SCHEMA GENERATION COMPLETED!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ CLAUDE-GENERATED SCHEMA FOR SAP_SALES_DATA\n",
      "======================================================================\n",
      "\n",
      "ğŸ—‚ï¸ DDL (Data Definition Language):\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "CREATE TABLE sap_sales_data (\n",
      "  `VBELN` INT NOT NULL,\n",
      "  `GPAG` DECIMAL(10,0) NOT NULL,\n",
      "  `JPARVWGPAG` BOOLEAN NOT NULL,\n",
      "  `ANGDT` DATE,\n",
      "  `BNDDT` DATE,\n",
      "  `VBTYP` VARCHAR(2) NOT NULL,\n",
      "  `TRVOG` INT,\n",
      "  `AUART` VARCHAR(4) NOT NULL,\n",
      "  `AUARTGRP` INT NOT NULL,\n",
      "  `WAERK` VARCHAR(5) NOT NULL,\n",
      "  `NETWR` DECIMAL(15,2) NOT NULL,\n",
      "  `VKORG` INT NOT NULL,\n",
      "  `VTWEG` VARCHAR(2) NOT NULL,\n",
      "  `SPART` VARCHAR(2) NOT NULL,\n",
      "  `VKGRP` VARCHAR(3),\n",
      "  `VKBUR` VARCHAR(4),\n",
      "  `EXEMPLART` VARCHAR(1) NOT NULL,\n",
      "  `GSBER` VARCHAR(4),\n",
      "  `GSKST` VARCHAR(4),\n",
      "  `KNUMV` INT NOT NULL,\n",
      "  `FAKSP` VARCHAR(2) NOT NULL,\n",
      "  `KALSM` VARCHAR(6) NOT NULL,\n",
      "  `KURST` VARCHAR(4) NOT NULL,\n",
      "  `BSTNK` VARCHAR(20) NOT NULL,\n",
      "  `BSARK` VARCHAR(1) NOT NULL,\n",
      "  `BSTDK` INT NOT NULL,\n",
      "  `BSTZD` VARCHAR(4),\n",
      "  `IHREZ` VARCHAR(3) NOT NULL,\n",
      "  `STAFO` VARCHAR(10),\n",
      "  `STWAE` VARCHAR(10),\n",
      "  `XIMMATRIK` VARCHAR(1) NOT NULL,\n",
      "  `IMMATDAT` INT NOT NULL,\n",
      "  `XSEPFKKOPF` VARCHAR(1) NOT NULL,\n",
      "  `XREMRECHT` VARCHAR(1) NOT NULL,\n",
      "  `REMSP` VARCHAR(10),\n",
      "  `KORRGRD` VARCHAR(10),\n",
      "  `REMVON` INT NOT NULL,\n",
      "  `REMBIS` INT NOT NULL,\n",
      "  `REFBELEG` VARCHAR(20) NOT NULL,\n",
      "  `REMDATUM` INT NOT NULL,\n",
      "  `REMSCHEIN` INT NOT NULL,\n",
      "  `IVWDATUM` INT NOT NULL,\n",
      "  `POSNR_LAST` INT NOT NULL,\n",
      "  `POSEX_LAST` INT NOT NULL,\n",
      "  `KPOSN_LAST` INT NOT NULL,\n",
      "  `XFKBASAUFT` VARCHAR(1) NOT NULL,\n",
      "  `XFKBASLIEF` VARCHAR(1) NOT NULL,\n",
      "  `XJKSOFAKT` VARCHAR(1),\n",
      "  `ERFUSER` VARCHAR(20) NOT NULL,\n",
      "  `ERFDATE` INT NOT NULL,\n",
      "  `ERFTIME` INT NOT NULL,\n",
      "  `AENUSER` VARCHAR(20) NOT NULL,\n",
      "  `AENDATE` INT NOT NULL,\n",
      "  `AENTIME` INT NOT NULL,\n",
      "  `XWBZABO` VARCHAR(4),\n",
      "  `XFKVDICHT` VARCHAR(4),\n",
      "  `XSTATARC` VARCHAR(4),\n",
      "  `XNOMESS` VARCHAR(4),\n",
      "  `KALSM_AMO` VARCHAR(4),\n",
      "  `XRENEWAL` VARCHAR(4),\n",
      "  `AMORTN` DECIMAL(15,2),\n",
      "  `REKLERGB` VARCHAR(2) NOT NULL,\n",
      "  `REKLTYP` INT NOT NULL,\n",
      "  `REKLDATUM` DATE NOT NULL\n",
      ");\n",
      "\n",
      "ğŸ“– Column Descriptions:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "{\n",
      "  \"VBELN\": \"The sales document number, a unique identifier for each sales order or delivery.\",\n",
      "  \"GPAG\": \"The sales document item number, representing a specific line item on the sales order or delivery.\",\n",
      "  \"JPARVWGPAG\": \"A flag indicating whether the sales document item is relevant for revenue recognition.\",\n",
      "  \"ANGDT\": \"The creation date of the sales document.\",\n",
      "  \"BNDDT\": \"The baseline date used for various calculations related to the sales document.\",\n",
      "  \"VBTYP\": \"The sales document category, indicating the type of document (e.g., order, delivery, invoice).\",\n",
      "  \"TRVOG\": \"The sales document scheduling line number, used for scheduling and logistics purposes.\",\n",
      "  \"AUART\": \"The sales document type, indicating the specific business process or transaction.\",\n",
      "  \"AUARTGRP\": \"The sales document type group, a higher-level categorization of the document type.\",\n",
      "  \"WAERK\": \"The currency code used for the sales document.\",\n",
      "  \"NETWR\": \"The net value of the sales document item, representing the amount after discounts and taxes.\",\n",
      "  \"VKORG\": \"The sales organization code, representing the organizational unit responsible for the sales process.\",\n",
      "  \"VTWEG\": \"The distribution channel code, indicating the sales channel or route to market.\",\n",
      "  \"SPART\": \"The product group code, representing a categorization of the products or services sold.\",\n",
      "  \"VKGRP\": \"The sales group code, used for grouping and analyzing sales data.\",\n",
      "  \"VKBUR\": \"The sales office code, representing the physical location or office responsible for the sales activities.\",\n",
      "  \"EXEMPLART\": \"A flag indicating the type of sales document, such as a sample or regular order.\",\n",
      "  \"GSBER\": \"The business area code, used for organizational or reporting purposes.\",\n",
      "  \"GSKST\": \"The cost center code, used for cost allocation and accounting purposes.\",\n",
      "  \"KNUMV\": \"The customer number, representing the unique identifier for the customer associated with the sales document.\",\n",
      "  \"FAKSP\": \"A two-character code representing the sales document type.\",\n",
      "  \"KALSM\": \"A six-character code identifying the condition type used for pricing or discounts.\",\n",
      "  \"KURST\": \"A four-character code representing the currency used for pricing.\",\n",
      "  \"BSTNK\": \"A reference number or identifier for the customer or billing entity.\",\n",
      "  \"BSARK\": \"A single-character code indicating the type of billing document or transaction.\",\n",
      "  \"BSTDK\": \"A numeric field potentially representing a discount or deduction amount.\",\n",
      "  \"BSTZD\": \"A four-character code potentially representing a time zone or time period.\",\n",
      "  \"IHREZ\": \"A three-character code potentially representing a user or employee identifier.\",\n",
      "  \"STAFO\": \"A field potentially representing a status or condition code.\",\n",
      "  \"STWAE\": \"A field potentially representing a status or condition code related to currency.\",\n",
      "  \"XIMMATRIK\": \"A single-character flag or indicator, potentially related to asset management.\",\n",
      "  \"IMMATDAT\": \"A numeric field potentially representing a date or timestamp related to asset management.\",\n",
      "  \"XSEPFKKOPF\": \"A single-character flag or indicator, potentially related to billing or invoicing.\",\n",
      "  \"XREMRECHT\": \"A single-character flag or indicator, potentially related to billing or invoicing rights.\",\n",
      "  \"REMSP\": \"A field potentially representing a code or identifier related to billing or invoicing.\",\n",
      "  \"KORRGRD\": \"A field potentially representing a reason code or identifier for corrections or adjustments.\",\n",
      "  \"REMVON\": \"A numeric field potentially representing a start date or period for billing or invoicing.\",\n",
      "  \"REMBIS\": \"A numeric field potentially representing an end date or period for billing or invoicing.\",\n",
      "  \"REFBELEG\": \"A reference number or identifier for a related document or transaction.\",\n",
      "  \"REMDATUM\": \"A numeric field potentially representing a date or timestamp related to billing or invoicing.\",\n",
      "  \"REMSCHEIN\": \"A numeric identifier or reference number for a sales order or delivery.\",\n",
      "  \"IVWDATUM\": \"A numeric representation of a date, likely related to sales order or delivery processing.\",\n",
      "  \"POSNR_LAST\": \"The last or most recent position or line item number associated with a sales order or delivery.\",\n",
      "  \"POSEX_LAST\": \"The last or most recent position or line item number for an external system or interface related to a sales order or delivery.\",\n",
      "  \"KPOSN_LAST\": \"The last or most recent position or line item number for a customer associated with a sales order or delivery.\",\n",
      "  \"XFKBASAUFT\": \"A flag or indicator related to sales order processing or fulfillment.\",\n",
      "  \"XFKBASLIEF\": \"A flag or indicator related to delivery processing or shipment.\",\n",
      "  \"XJKSOFAKT\": \"A flag or indicator related to sales order invoicing or billing.\",\n",
      "  \"ERFUSER\": \"The user or person who created or entered the sales order or delivery record.\",\n",
      "  \"ERFDATE\": \"The date when the sales order or delivery record was created or entered, represented as an integer.\",\n",
      "  \"ERFTIME\": \"The time when the sales order or delivery record was created or entered, represented as an integer.\",\n",
      "  \"AENUSER\": \"The user or person who last modified or updated the sales order or delivery record.\",\n",
      "  \"AENDATE\": \"The date when the sales order or delivery record was last modified or updated, represented as an integer.\",\n",
      "  \"AENTIME\": \"The time when the sales order or delivery record was last modified or updated, represented as an integer.\",\n",
      "  \"XWBZABO\": \"A flag or indicator related to recurring or subscription-based sales orders or deliveries.\",\n",
      "  \"XFKVDICHT\": \"A flag or indicator related to sales order or delivery density or consolidation.\",\n",
      "  \"XSTATARC\": \"A flag or indicator related to the archiving or retention status of sales order or delivery records.\",\n",
      "  \"XNOMESS\": \"A flag or indicator related to messaging or notifications for sales orders or deliveries.\",\n",
      "  \"KALSM_AMO\": \"A code or identifier related to sales order or delivery pricing or revenue recognition.\",\n",
      "  \"XRENEWAL\": \"A flag or indicator related to the renewal or extension of sales orders or deliveries.\",\n",
      "  \"AMORTN\": \"Represents the amortization amount or value related to sales or financial transactions.\",\n",
      "  \"REKLERGB\": \"A two-character code representing the result or outcome of a sales or marketing campaign.\",\n",
      "  \"REKLTYP\": \"A numeric code representing the type of sales or marketing campaign.\",\n",
      "  \"REKLDATUM\": \"The date associated with a sales or marketing campaign, likely the start or end date.\"\n",
      "}\n",
      "\n",
      "======================================================================\n",
      "\n",
      "âœ… Successfully generated schema for 64 columns\n",
      "âœ… DDL ready for database implementation\n",
      "\n",
      "ğŸ“ Sample column descriptions:\n",
      "   1. VBELN: The sales document number, a unique identifier for each sales order or delivery....\n",
      "   2. GPAG: The sales document item number, representing a specific line item on the sales order or delivery....\n",
      "   3. JPARVWGPAG: A flag indicating whether the sales document item is relevant for revenue recognition....\n",
      "   4. ANGDT: The creation date of the sales document....\n",
      "   5. BNDDT: The baseline date used for various calculations related to the sales document....\n",
      "\n",
      "ğŸ Improved SAP Schema Generation Test Complete\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED SAP Dataset Schema Generation with Enhanced Error Handling\n",
    "print(\"ğŸš€ IMPROVED SAP DATASET SCHEMA GENERATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Enhanced function to handle large datasets\n",
    "def analyze_large_dataset_schema(df: pl.DataFrame, table_name: str, max_columns_per_batch: int = 30) -> Tuple[str, str]:\n",
    "    \"\"\"Analyze schema for large datasets by processing in batches if needed\"\"\"\n",
    "    \n",
    "    total_columns = len(df.columns)\n",
    "    print(f\"ğŸ“Š Dataset: {table_name} with {total_columns} columns\")\n",
    "    \n",
    "    if total_columns <= max_columns_per_batch:\n",
    "        # Process all columns at once\n",
    "        return analyze_schema_with_claude(df, table_name, sample_size=3)\n",
    "    else:\n",
    "        # Process in batches\n",
    "        print(f\"âš ï¸ Large dataset detected ({total_columns} columns). Processing in batches...\")\n",
    "        \n",
    "        all_schemas = {}\n",
    "        batch_num = 1\n",
    "        \n",
    "        for i in range(0, total_columns, max_columns_per_batch):\n",
    "            batch_columns = df.columns[i:i + max_columns_per_batch]\n",
    "            batch_df = df.select(batch_columns)\n",
    "            \n",
    "            print(f\"\\nğŸ”„ Processing batch {batch_num}: columns {i+1}-{min(i+max_columns_per_batch, total_columns)}\")\n",
    "            \n",
    "            # Extract samples for this batch\n",
    "            batch_samples = extract_column_samples(batch_df, sample_size=3)\n",
    "            \n",
    "            # Create prompt for this batch\n",
    "            batch_prompt = prepare_claude_prompt(batch_samples, f\"{table_name}_batch_{batch_num}\")\n",
    "            \n",
    "            # Call Claude API\n",
    "            print(f\"ğŸ¤– Calling Claude API for batch {batch_num}...\")\n",
    "            batch_schema = call_claude_api(batch_prompt)\n",
    "            \n",
    "            if batch_schema and 'columns' in batch_schema:\n",
    "                all_schemas.update(batch_schema['columns'])\n",
    "                print(f\"âœ… Batch {batch_num} completed successfully\")\n",
    "            else:\n",
    "                print(f\"âŒ Batch {batch_num} failed\")\n",
    "            \n",
    "            batch_num += 1\n",
    "        \n",
    "        # Combine all schemas\n",
    "        combined_schema = {'columns': all_schemas}\n",
    "        ddl = generate_ddl_from_schema(combined_schema, table_name)\n",
    "        descriptions = generate_column_descriptions(combined_schema)\n",
    "        \n",
    "        return ddl, descriptions\n",
    "\n",
    "# Test with SAP dataset\n",
    "if 'sap' in datasets:\n",
    "    print(\"\\nğŸ¯ Testing improved SAP dataset schema generation...\")\n",
    "    \n",
    "    # Test Claude connection first\n",
    "    connection_ok = test_claude_connection()\n",
    "    \n",
    "    if connection_ok:\n",
    "        try:\n",
    "            sap_df = datasets['sap']\n",
    "            print(f\"\\nğŸ“ˆ SAP Dataset: {sap_df.shape[0]:,} rows Ã— {sap_df.shape[1]} columns\")\n",
    "            \n",
    "            # Use the improved function\n",
    "            ddl, descriptions = analyze_large_dataset_schema(sap_df, 'sap_sales_data', max_columns_per_batch=20)\n",
    "            \n",
    "            # Display results\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"ğŸ‰ IMPROVED SAP SCHEMA GENERATION COMPLETED!\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            display_schema_results(ddl, descriptions, 'sap_sales_data')\n",
    "            \n",
    "            # Validation\n",
    "            try:\n",
    "                desc_data = json.loads(descriptions)\n",
    "                print(f\"\\nâœ… Successfully generated schema for {len(desc_data)} columns\")\n",
    "                print(f\"âœ… DDL ready for database implementation\")\n",
    "                \n",
    "                # Show sample of descriptions\n",
    "                print(f\"\\nğŸ“ Sample column descriptions:\")\n",
    "                for i, (col, desc) in enumerate(list(desc_data.items())[:5]):\n",
    "                    print(f\"   {i+1}. {col}: {desc[:100]}...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Could not parse descriptions: {e}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error in improved schema generation: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(\"âŒ Claude API connection failed\")\n",
    "else:\n",
    "    print(\"âŒ SAP dataset not found\")\n",
    "\n",
    "print(\"\\nğŸ Improved SAP Schema Generation Test Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473f5597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPY-PASTE FORMATTING FUNCTIONS\n",
    "print(\"ğŸ“‹ COPY-PASTE FORMATTING FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def format_for_copy_paste(ddl: str, descriptions: str, table_name: str, source_file: str = None) -> str:\n",
    "    \"\"\"Format results for easy copy-paste with timestamps and metadata\"\"\"\n",
    "    \n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    output = f\"\"\"-- =========================================\n",
    "-- SCAVENGER AI - GENERATED SCHEMA\n",
    "-- =========================================\n",
    "-- Table Name: {table_name.upper()}\n",
    "-- Generated: {timestamp}\n",
    "-- Source File: {source_file if source_file else 'Unknown'}\n",
    "-- Tool: Scavenger AI Schema Generator v1.0\n",
    "-- =========================================\n",
    "\n",
    "{ddl}\n",
    "\n",
    "-- =========================================\n",
    "-- COLUMN DESCRIPTIONS\n",
    "-- =========================================\n",
    "/*\n",
    "Generated Column Descriptions (JSON Format):\n",
    "\n",
    "{descriptions}\n",
    "*/\n",
    "\n",
    "-- =========================================\n",
    "-- IMPLEMENTATION NOTES\n",
    "-- =========================================\n",
    "/*\n",
    "1. Review data types for optimization\n",
    "2. Add indexes as needed for performance\n",
    "3. Consider adding primary key constraints\n",
    "4. Validate NOT NULL constraints with business rules\n",
    "5. Add foreign key relationships if applicable\n",
    "*/\n",
    "\"\"\"\n",
    "    return output\n",
    "\n",
    "def format_ddl_only(ddl: str, table_name: str) -> str:\n",
    "    \"\"\"Format only DDL for quick implementation\"\"\"\n",
    "    \n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    output = f\"\"\"-- Generated by Scavenger AI on {timestamp}\n",
    "-- Table: {table_name.upper()}\n",
    "\n",
    "{ddl}\n",
    "\"\"\"\n",
    "    return output\n",
    "\n",
    "def format_descriptions_only(descriptions: str, table_name: str) -> str:\n",
    "    \"\"\"Format only descriptions for documentation\"\"\"\n",
    "    \n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    output = f\"\"\"-- COLUMN DESCRIPTIONS FOR {table_name.upper()}\n",
    "-- Generated by Scavenger AI on {timestamp}\n",
    "\n",
    "{descriptions}\n",
    "\"\"\"\n",
    "    return output\n",
    "\n",
    "def generate_implementation_guide(ddl: str, descriptions: str, table_name: str) -> str:\n",
    "    \"\"\"Generate implementation guide with best practices\"\"\"\n",
    "    \n",
    "    try:\n",
    "        desc_data = json.loads(descriptions)\n",
    "        column_count = len(desc_data)\n",
    "    except:\n",
    "        column_count = ddl.count('`') // 2  # Estimate from backticks\n",
    "    \n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    guide = f\"\"\"-- IMPLEMENTATION GUIDE FOR {table_name.upper()}\n",
    "-- Generated: {timestamp}\n",
    "-- Columns: {column_count}\n",
    "\n",
    "-- 1. CREATE TABLE STATEMENT\n",
    "{ddl}\n",
    "\n",
    "-- 2. RECOMMENDED INDEXES\n",
    "-- Add these after table creation for better performance:\n",
    "/*\n",
    "CREATE INDEX idx_{table_name}_created ON {table_name} (created_at);\n",
    "-- Add more indexes based on your query patterns\n",
    "*/\n",
    "\n",
    "-- 3. SAMPLE INSERT STATEMENT\n",
    "/*\n",
    "INSERT INTO {table_name} (\n",
    "  -- List your columns here\n",
    ") VALUES (\n",
    "  -- Provide sample values\n",
    ");\n",
    "*/\n",
    "\n",
    "-- 4. SAMPLE SELECT STATEMENTS\n",
    "/*\n",
    "-- Basic select\n",
    "SELECT * FROM {table_name} LIMIT 10;\n",
    "\n",
    "-- Count records\n",
    "SELECT COUNT(*) FROM {table_name};\n",
    "\n",
    "-- Check data quality\n",
    "SELECT \n",
    "  COUNT(*) as total_rows,\n",
    "  COUNT(*) - COUNT(column_name) as nulls_in_column\n",
    "FROM {table_name};\n",
    "*/\n",
    "\n",
    "-- 5. COLUMN DESCRIPTIONS\n",
    "{descriptions}\n",
    "\"\"\"\n",
    "    return guide\n",
    "\n",
    "def save_formatted_output(ddl: str, descriptions: str, table_name: str, output_type: str = \"complete\"):\n",
    "    \"\"\"Save formatted output to file (simulated for notebook)\"\"\"\n",
    "    \n",
    "    if output_type == \"complete\":\n",
    "        content = format_for_copy_paste(ddl, descriptions, table_name)\n",
    "        filename = f\"{table_name}_complete_schema.sql\"\n",
    "    elif output_type == \"ddl\":\n",
    "        content = format_ddl_only(ddl, table_name)\n",
    "        filename = f\"{table_name}_ddl.sql\"\n",
    "    elif output_type == \"descriptions\":\n",
    "        content = format_descriptions_only(descriptions, table_name)\n",
    "        filename = f\"{table_name}_descriptions.json\"\n",
    "    elif output_type == \"guide\":\n",
    "        content = generate_implementation_guide(ddl, descriptions, table_name)\n",
    "        filename = f\"{table_name}_implementation_guide.sql\"\n",
    "    else:\n",
    "        print(f\"âŒ Unknown output type: {output_type}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ğŸ“ Generated {output_type} format for {table_name}\")\n",
    "    print(f\"ğŸ’¾ Filename: {filename}\")\n",
    "    print(f\"ğŸ“Š Size: {len(content)} characters\")\n",
    "    \n",
    "    # In a real implementation, you would save to file:\n",
    "    # with open(filename, 'w') as f:\n",
    "    #     f.write(content)\n",
    "    \n",
    "    return content, filename\n",
    "\n",
    "def display_copy_paste_options(ddl: str, descriptions: str, table_name: str):\n",
    "    \"\"\"Display all copy-paste options with clear formatting\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ COPY-PASTE OPTIONS FOR {table_name.upper()}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Option 1: Complete Package\n",
    "    print(\"\\nğŸ“¦ 1. COMPLETE PACKAGE (DDL + Descriptions + Metadata)\")\n",
    "    print(\"â”€\" * 60)\n",
    "    complete_output = format_for_copy_paste(ddl, descriptions, table_name)\n",
    "    print(complete_output)\n",
    "    \n",
    "    # Option 2: DDL Only\n",
    "    print(\"\\n\\nğŸ”§ 2. DDL ONLY (Quick Implementation)\")\n",
    "    print(\"â”€\" * 60)\n",
    "    ddl_output = format_ddl_only(ddl, table_name)\n",
    "    print(ddl_output)\n",
    "    \n",
    "    # Option 3: Implementation Guide\n",
    "    print(\"\\n\\nğŸ“– 3. IMPLEMENTATION GUIDE (Best Practices)\")\n",
    "    print(\"â”€\" * 60)\n",
    "    guide_output = generate_implementation_guide(ddl, descriptions, table_name)\n",
    "    print(guide_output)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"âœ… All formats ready for copy-paste!\")\n",
    "    \n",
    "    return {\n",
    "        'complete': complete_output,\n",
    "        'ddl_only': ddl_output,\n",
    "        'implementation_guide': guide_output\n",
    "    }\n",
    "\n",
    "print(\"âœ… Copy-paste formatting functions loaded successfully!\")\n",
    "print(\"Available functions:\")\n",
    "print(\"  - format_for_copy_paste()\")\n",
    "print(\"  - format_ddl_only()\")\n",
    "print(\"  - format_descriptions_only()\")\n",
    "print(\"  - generate_implementation_guide()\")\n",
    "print(\"  - display_copy_paste_options()\")\n",
    "print(\"  - save_formatted_output()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED SCHEMA GENERATION WITH AUTO-BATCHING (20 COLUMNS MAX)\n",
    "print(\"ğŸ¯ OPTIMIZED AUTO-BATCHING SCHEMA GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def generate_schema_with_auto_batching(df: pl.DataFrame, table_name: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Generate schema with automatic batching for datasets.\n",
    "    - Small datasets (â‰¤20 columns): Single batch processing\n",
    "    - Large datasets (>20 columns): Automatic 20-column batches\n",
    "    \"\"\"\n",
    "    \n",
    "    total_columns = len(df.columns)\n",
    "    BATCH_SIZE = 20\n",
    "    \n",
    "    print(f\"ğŸ“Š Dataset '{table_name}': {df.shape[0]:,} rows Ã— {total_columns} columns\")\n",
    "    \n",
    "    if total_columns <= BATCH_SIZE:\n",
    "        # Small dataset - process all at once\n",
    "        print(f\"âœ… Small dataset detected. Processing all {total_columns} columns in single batch.\")\n",
    "        return analyze_schema_with_claude(df, table_name, sample_size=5)\n",
    "    \n",
    "    else:\n",
    "        # Large dataset - process in batches of 20\n",
    "        num_batches = (total_columns + BATCH_SIZE - 1) // BATCH_SIZE  # Ceiling division\n",
    "        print(f\"ğŸ“¦ Large dataset detected. Processing in {num_batches} batches of {BATCH_SIZE} columns each.\")\n",
    "        \n",
    "        all_schemas = {}\n",
    "        \n",
    "        for batch_num in range(1, num_batches + 1):\n",
    "            start_idx = (batch_num - 1) * BATCH_SIZE\n",
    "            end_idx = min(start_idx + BATCH_SIZE, total_columns)\n",
    "            batch_columns = df.columns[start_idx:end_idx]\n",
    "            \n",
    "            print(f\"\\nğŸ”„ Batch {batch_num}/{num_batches}: Processing columns {start_idx + 1}-{end_idx}\")\n",
    "            print(f\"   ğŸ“ Columns: {list(batch_columns)}\")\n",
    "            \n",
    "            # Create batch DataFrame\n",
    "            batch_df = df.select(batch_columns)\n",
    "            \n",
    "            # Extract samples for this batch\n",
    "            batch_samples = extract_column_samples(batch_df, sample_size=5)\n",
    "            \n",
    "            # Create prompt for this batch\n",
    "            batch_prompt = prepare_claude_prompt(batch_samples, f\"{table_name}_batch_{batch_num}\")\n",
    "            \n",
    "            # Call Claude API\n",
    "            print(f\"   ğŸ¤– Calling Claude API...\")\n",
    "            batch_schema = call_claude_api(batch_prompt)\n",
    "            \n",
    "            if batch_schema and 'columns' in batch_schema:\n",
    "                all_schemas.update(batch_schema['columns'])\n",
    "                print(f\"   âœ… Batch {batch_num} completed successfully ({len(batch_schema['columns'])} columns processed)\")\n",
    "            else:\n",
    "                print(f\"   âŒ Batch {batch_num} failed - skipping\")\n",
    "        \n",
    "        # Combine all schemas\n",
    "        print(f\"\\nğŸ”— Combining results from all batches...\")\n",
    "        combined_schema = {'columns': all_schemas}\n",
    "        ddl = generate_ddl_from_schema(combined_schema, table_name)\n",
    "        descriptions = generate_column_descriptions(combined_schema)\n",
    "        \n",
    "        print(f\"âœ… Successfully processed {len(all_schemas)} columns across {num_batches} batches\")\n",
    "        return ddl, descriptions\n",
    "\n",
    "# Test the optimized function with both datasets\n",
    "print(\"\\nğŸ§ª TESTING AUTO-BATCHING FUNCTION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test Claude connection first\n",
    "connection_ok = test_claude_connection()\n",
    "\n",
    "if connection_ok and 'datasets' in globals() and datasets:\n",
    "    \n",
    "    # Test with Insurance dataset (small - should be single batch)\n",
    "    if 'insurance' in datasets:\n",
    "        print(\"\\nğŸ“‹ Testing with Insurance Dataset (Small):\")\n",
    "        print(\"=\" * 45)\n",
    "        try:\n",
    "            insurance_ddl, insurance_desc = generate_schema_with_auto_batching(datasets['insurance'], 'insurance_data')\n",
    "            display_schema_results(insurance_ddl, insurance_desc, 'insurance_data')\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error with insurance dataset: {e}\")\n",
    "    \n",
    "    # Test with SAP dataset (large - should be multi-batch)\n",
    "    if 'sap' in datasets:\n",
    "        print(\"\\nğŸ“‹ Testing with SAP Dataset (Large):\")\n",
    "        print(\"=\" * 40)\n",
    "        try:\n",
    "            sap_ddl, sap_desc = generate_schema_with_auto_batching(datasets['sap'], 'sap_sales_data')\n",
    "            display_schema_results(sap_ddl, sap_desc, 'sap_sales_data')\n",
    "            \n",
    "            # Show summary statistics\n",
    "            try:\n",
    "                desc_data = json.loads(sap_desc)\n",
    "                print(f\"\\nğŸ“ˆ FINAL SUMMARY:\")\n",
    "                print(f\"   âœ… Total columns processed: {len(desc_data)}\")\n",
    "                print(f\"   âœ… DDL lines generated: {sap_ddl.count('`')}\")\n",
    "                print(f\"   âœ… Schema ready for implementation\")\n",
    "                \n",
    "                # Show first few column descriptions as examples\n",
    "                print(f\"\\nğŸ“ Sample Column Descriptions:\")\n",
    "                for i, (col, desc) in enumerate(list(desc_data.items())[:3]):\n",
    "                    print(f\"   {i+1}. {col}: {desc[:80]}...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Could not parse final descriptions: {e}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error with SAP dataset: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "else:\n",
    "    if not connection_ok:\n",
    "        print(\"âŒ Claude API connection failed\")\n",
    "    else:\n",
    "        print(\"âŒ No datasets available for testing\")\n",
    "\n",
    "print(\"\\nğŸ Auto-Batching Schema Generation Test Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb39b0",
   "metadata": {},
   "source": [
    "## Component Three: Auto-Batching Schema Generation\n",
    "\n",
    "### Goal\n",
    "Intelligently generate SQL schemas and column descriptions using Claude AI with automatic batch processing for optimal performance.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Smart Batching Logic:**\n",
    "  - Small datasets (â‰¤20 columns): Single batch processing\n",
    "  - Large datasets (>20 columns): Automatic 20-column batches\n",
    "  \n",
    "- **Comprehensive Output:**\n",
    "  - Production-ready DDL (Data Definition Language)\n",
    "  - Professional column descriptions in JSON format\n",
    "  - Progress tracking and error handling\n",
    "\n",
    "### Technology Stack\n",
    "\n",
    "- **Claude AI API:** Advanced schema analysis and type inference\n",
    "- **Polars:** High-performance data sampling\n",
    "- **Batch Processing:** Optimized for large datasets like SAP (64 columns)\n",
    "\n",
    "### Success Criteria\n",
    "\n",
    "- âœ… Accurate SQL type mapping (INT, VARCHAR, DECIMAL, etc.)\n",
    "- âœ… Professional business-oriented column descriptions\n",
    "- âœ… Reliable processing of both small and large datasets\n",
    "- âœ… Database-ready DDL generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
